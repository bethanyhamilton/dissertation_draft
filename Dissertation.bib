@article{ahn2012,
  title = {A {{Review}} of {{Meta-Analyses}} in {{Education}}: {{Methodological Strengths}} and {{Weaknesses}}},
  shorttitle = {A {{Review}} of {{Meta-Analyses}} in {{Education}}},
  author = {Ahn, Soyeon and Ames, Allison J. and Myers, Nicholas D.},
  year = {2012},
  journal = {Review of Educational Research},
  volume = {82},
  number = {4},
  eprint = {41812114},
  eprinttype = {jstor},
  pages = {436--476},
  publisher = {[Sage Publications, Inc., American Educational Research Association]},
  issn = {0034-6543},
  urldate = {2023-07-17},
  abstract = {The current review addresses the validity of published meta-analyses in education that determines the credibility and generalizability of study findings using a total of 56 meta-analyses published in education in the 2000s. Our objectives were to evaluate the current meta-analytic practices in education, identify methodological strengths and weaknesses, and provide recommendations for improvements in order to generate a more valid and credible knowledge base of what works in practice. It was found that 56 meta-analyses followed general recommendations fairly well in problem formulation and data collection, but much improvement is needed in data evaluation and analysis. Particularly, lack of information reported as well as little transparency in the use of statistical methods are concerns for generating credible and generalizable meta-analytic findings that can be transformed to educational practices. Recommendations for yielding more reliable and valid inferences from meta-analyses are provided.},
  file = {C:\Users\micro\Zotero\storage\3HUJZLEZ\Ahn et al. - 2012 - A Review of Meta-Analyses in Education Methodolog.pdf}
}

@book{authors,
  title = {Designing {{Monte Carlo Simulations}} in {{R}}},
  author = {{authors)}, Luke W. Miratrix {and} James E. Pustejovsky (Equal},
  urldate = {2024-10-28},
  abstract = {A text on designing, implementing, and reporting on Monte Carlo simulation studies},
  file = {C:\Users\micro\Zotero\storage\U9NUFG36\Designing-Simulations-in-R.html}
}

@book{borenstein2021,
  title = {Introduction to {{Meta-Analysis}}},
  author = {Borenstein, Michael and Hedges, Larry V. and Higgins, Julian P. T. and Rothstein, Hannah R.},
  year = {2021},
  month = apr,
  publisher = {John Wiley \& Sons},
  abstract = {A clear and thorough introduction to meta-analysis, the process of synthesizing data from a series of separate studies The first edition of this text was widely acclaimed for the clarity of the presentation, and quickly established itself as the definitive text in this field. The fully updated second edition includes new and expanded content on avoiding common mistakes in meta-analysis, understanding heterogeneity in effects, publication bias, and more. Several brand-new chapters provide a systematic "how to" approach to performing and reporting a meta-analysis from start to finish. Written by four of the world's foremost authorities on all aspects of meta-analysis, the new edition:  Outlines the role of meta-analysis in the research process Shows how to compute effects sizes and treatment effects Explains the fixed-effect and random-effects models for synthesizing data Demonstrates how to assess and interpret variation in effect size across studies Explains how to avoid common mistakes in meta-analysis Discusses controversies in meta-analysis Includes access to a companion website containing videos, spreadsheets, data files, free software for prediction intervals, and step-by-step instructions for performing analyses using Comprehensive Meta-Analysis (CMA)  Download videos, class materials, and worked examples at www.Introduction-to-Meta-Analysis.com "This book offers the reader a unified framework for thinking about meta-analysis, and then discusses all elements of the analysis within that framework. The authors address a series of common mistakes and explain how to avoid them. As the editor-in-chief of the American Psychologist and former editor of Psychological Bulletin, I can say without hesitation that the quality of manuscript submissions reporting meta-analyses would be vastly better if researchers read this book."---Harris Cooper, Hugo L. Blomquist Distinguished Professor Emeritus of Psychology and Neuroscience, Editor-in-chief of the American Psychologist, former editor of Psychological Bulletin "A superb combination of lucid prose and informative graphics, the authors provide a refreshing departure from cookbook approaches with their clear explanations of the what and why of meta-analysis. The book is ideal as a course textbook or for self-study. My students raved about the clarity of the explanations and examples." ---David Rindskopf, Distinguished Professor of Educational Psychology, City University of New York, Graduate School and University Center, \& Editor of the Journal of Educational and Behavioral Statistics "The approach taken by Introduction to Meta-analysis is intended to be primarily conceptual, and it is amazingly successful at achieving that goal. The reader can comfortably skip the formulas and still understand their application and underlying motivation. For the more statistically sophisticated reader, the relevant formulas and worked examples provide a superb practical guide to performing a meta-analysis. The book provides an eclectic mix of examples from education, social science, biomedical studies, and even ecology. For anyone considering leading a course in meta-analysis, or pursuing self-directed study, Introduction to Meta-analysis would be a clear first choice." ---Jesse A. Berlin, ScD},
  googlebooks = {pdQnEAAAQBAJ},
  isbn = {978-1-119-55838-5},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Medical / Biostatistics}
}

@book{brent2013,
  title = {Algorithms for {{Minimization Without Derivatives}}},
  author = {Brent, Richard P.},
  year = {2013},
  month = jun,
  publisher = {Courier Corporation},
  abstract = {This outstanding text for graduate students and researchers proposes improvements to existing algorithms, extends their related mathematical theories, and offers details on new algorithms for approximating local and global minima. None of the algorithms requires an evaluation of derivatives; all depend entirely on sequential function evaluation, a highly practical scenario in the frequent event of difficult-to-evaluate derivatives.Topics include the use of successive interpolation for finding simple zeros of a function and its derivatives; an algorithm with guaranteed convergence for finding a minimum of a function of one variation; global minimization given an upper bound on the second derivative; and a new algorithm for minimizing a function of several variables without calculating derivatives. Many numerical examples augment the text, along with a complete analysis of rate of convergence for most algorithms and error bounds that allow for the effect of rounding errors.},
  googlebooks = {AITCAgAAQBAJ},
  isbn = {978-0-486-14368-2},
  langid = {english},
  keywords = {Mathematics / Optimization}
}

@article{cochran1954,
  title = {The {{Combination}} of {{Estimates}} from {{Different Experiments}}},
  author = {Cochran, William G.},
  year = {1954},
  journal = {Biometrics},
  volume = {10},
  number = {1},
  eprint = {3001666},
  eprinttype = {jstor},
  pages = {101--129},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/3001666},
  urldate = {2024-10-26},
  file = {C:\Users\micro\Zotero\storage\LQQFJMJ9\Cochran - 1954 - The Combination of Estimates from Different Experi.pdf}
}

@book{cooper2019,
  title = {The {{Handbook}} of {{Research Synthesis}} and {{Meta-Analysis}}},
  author = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey C.},
  year = {2019},
  month = jun,
  publisher = {Russell Sage Foundation},
  abstract = {Research synthesis is the practice of systematically distilling and integrating data from many studies in order to draw more reliable conclusions about a given research issue. When the first edition of The Handbook of Research Synthesis and Meta-Analysis was published in 1994, it quickly became the definitive reference for conducting meta-analyses in both the social and behavioral sciences. In the third edition, editors Harris Cooper, Larry Hedges, and Jeff Valentine present updated versions of classic chapters and add new sections that evaluate cutting-edge developments in the field.  The Handbook of Research Synthesis and Meta-Analysis draws upon groundbreaking advances that have transformed research synthesis from a narrative craft into an important scientific process in its own right. The editors and leading scholars guide the reader through every stage of the research synthesis process---problem formulation, literature search and evaluation, statistical integration, and report preparation. The Handbook incorporates state-of-the-art techniques from all quantitative synthesis traditions and distills a vast literature to explain the most effective solutions to the problems of quantitative data integration. Among the statistical issues addressed are the synthesis of non-independent data sets, fixed and random effects methods, the performance of sensitivity analyses and model assessments, the development of machine-based abstract screening, the increased use of meta-regression and the problems of missing data. The Handbook also addresses the non-statistical aspects of research synthesis, including searching the literature and developing schemes for gathering information from study reports. Those engaged in research synthesis will find useful advice on how tables, graphs, and narration can foster communication of the results of research syntheses.  The third edition of the Handbook provides comprehensive instruction in the skills necessary to conduct research syntheses and represents the premier text on research synthesis.   Praise for the first edition: "The Handbook is a comprehensive treatment of literature synthesis and provides practical advice for anyone deep in the throes of, just teetering on the brink of, or attempting to decipher a meta-analysis. Given the expanding application and importance of literature synthesis, understanding both its strengths and weaknesses is essential for its practitioners and consumers. This volume is a good beginning for those who wish to gain that understanding." ---Chance "Meta-analysis, as the statistical analysis of a large collection of results from individual studies is called, has now achieved a status of respectability in medicine. This respectability, when combined with the slight hint of mystique that sometimes surrounds meta-analysis, ensures that results of studies that use it are treated with the respect they deserve....The Handbook of Research Synthesis is one of the most important publications in this subject both as a definitive reference book and a practical manual."---British Medical Journal When the first edition of The Handbook of Research Synthesis was published in 1994, it quickly became the definitive reference for researchers conducting meta-analyses of existing research in both the social and biological sciences. In this fully revised second edition, editors Harris Cooper, Larry Hedges, and Jeff Valentine present updated versions of the Handbook's classic chapters, as well as entirely new sections reporting on the most recent, cutting-edge developments in the field. Research synthesis is the practice of systematically distilling and integrating data from a variety of sources in order to draw more reliable conclusions about a given question or topic. The Handbook of Research Synthesis and Meta-Analysis draws upon years of groundbreaking advances that have transformed research synthesis from a narrative craft into an important scientific process in its own right. Cooper, Hedges, and Valentine have assembled leading authorities in the field to guide the reader through every stage of the research synthesis process---problem formulation, literature search and evaluation, statistical integration, and report preparation. The Handbook of Research Synthesis and Meta-Analysis incorporates state-of-the-art techniques from all quantitative synthesis traditions. Distilling a vast technical literature and many informal sources, the Handbook provides a portfolio of the most effective solutions to the problems of quantitative data integration. Among the statistical issues addressed by the authors are the synthesis of non-independent data sets, fixed and random effects methods, the performance of sensitivity analyses and model assessments, and the problem of missing data. The Handbook of Research Synthesis and Meta-Analysis also provides a rich treatment of the non-statistical aspects of research synthesis. Topics include searching the literature, and developing schemes for gathering information from study reports. Those engaged in research synthesis will also find useful advice on how tables, graphs, and narration can be used to provide the most meaningful communication of the results of research synthesis. In addition, the editors address the potentials and limitations of research synthesis, and its future directions. The past decade has been a period of enormous growth in the field of research synthesis. The second edition Handbook thoroughly revises original chapters to assure that the volume remains the most authoritative source of information for researchers undertaking meta-analysis today. In response to the increasing use of research synthesis in the formation of public policy, the second edition includes a new chapter on both the strengths and limitations of research synthesis in policy debates},
  googlebooks = {tfeXDwAAQBAJ},
  isbn = {978-1-61044-886-4},
  langid = {english},
  keywords = {Psychology / Experimental Psychology,Social Science / General,Social Science / Methodology,Social Science / Research}
}

@article{hedges2001,
  title = {The Power of Statistical Tests in Meta-Analysis},
  author = {Hedges, Larry V. and Pigott, Therese D.},
  year = {2001},
  month = sep,
  journal = {Psychological Methods},
  volume = {6},
  number = {3},
  pages = {203--217},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.6.3.203},
  urldate = {2023-07-14},
  abstract = {Calculations of the power of statistical tests are important in planning research studies (including meta-analyses) and in interpreting situations in which a result has not proven to be statistically significant. The authors describe procedures to compute statistical power of fixed- and random-effects tests of the mean effect size, tests for heterogeneity (or variation) of effect size parameters across studies, and tests for contrasts among effect sizes of different studies. Examples are given using 2 published meta-analyses. The examples illustrate that statistical power is not always high in meta-analysis. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {{Models, Psychological},Humans,Meta Analysis,meta-analysis,Meta-Analysis as Topic,power,Statistical Power,statistical tests,Statistical Tests},
  file = {C:\Users\micro\Zotero\storage\ATT2C9S6\Hedges and Pigott - 2001 - The power of statistical tests in meta-analysis.pdf}
}

@article{hedges2004,
  title = {The {{Power}} of {{Statistical Tests}} for {{Moderators}} in {{Meta-Analysis}}.},
  author = {Hedges, Larry V. and Pigott, Therese D.},
  year = {2004},
  journal = {Psychological Methods},
  volume = {9},
  number = {4},
  pages = {426--445},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.4.426},
  urldate = {2023-07-17},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\JNH58GD2\Hedges and Pigott - 2004 - The Power of Statistical Tests for Moderators in M.pdf}
}


@article{hedges2013,
	title = {Intraclass {Correlations} and {Covariate} {Outcome} {Correlations} for {Planning} {Two}- and {Three}-{Level} {Cluster}-{Randomized} {Experiments} in {Education}},
	volume = {37},
	issn = {0193-841X},
	url = {https://doi.org/10.1177/0193841X14529126},
	doi = {10.1177/0193841X14529126},
	abstract = {Background:Cluster-randomized experiments that assign intact groups such as schools or school districts to treatment conditions are increasingly common in educational research. Such experiments are inherently multilevel designs whose sensitivity (statistical power and precision of estimates) depends on the variance decomposition across levels. This variance decomposition is usually summarized by the intraclass correlation (ICC) structure and, if covariates are used, the effectiveness of the covariates in explaining variation at each level of the design.Objectives:This article provides a compilation of school- and district-level ICC values of academic achievement and related covariate effectiveness based on state longitudinal data systems. These values are designed to be used for planning group-randomized experiments in education. The use of these values to compute statistical power and plan two- and three-level group-randomized experiments is illustrated.Research Design:We fit several hierarchical linear models to state data by grade and subject to estimate ICCs and covariate effectiveness. The total sample size is over 4.8 million students. We then compare our average of state estimates with the national work by Hedges and Hedberg.},
	number = {6},
	urldate = {2021-05-29},
	journal = {Evaluation Review},
	author = {Hedges, Larry V. and Hedberg, E. C.},
	month = dec,
	year = {2013},
	note = {Publisher: SAGE Publications Inc},
	pages = {445--489},
	file = {SAGE PDF Full Text:C\:\\Users\\betha\\Zotero\\storage\\9L2HX6QH\\Hedges and Hedberg - 2013 - Intraclass Correlations and Covariate Outcome Corr.pdf:application/pdf},
}

@article{hedgesEffectSizesClusterRandomized2007,
	title = {Effect {Sizes} in {Cluster}-{Randomized} {Designs}},
	volume = {32},
	issn = {1076-9986},
	url = {https://doi.org/10.3102/1076998606298043},
	doi = {10.3102/1076998606298043},
	abstract = {Multisite research designs involving cluster randomization are becoming increasingly important in educational and behavioral research. Researchers would like to compute effect size indexes based on the standardized mean difference to compare the results of cluster-randomized studies (and corresponding quasi-experiments) with other studies and to combine information across studies in meta-analyses. This article addresses the problem of defining effect sizes in multilevel designs and computing estimates of those effect sizes and their standard errors from information that is likely to be reported in journal articles. Three effect sizes are defined corresponding to different standardizations. Estimators of each effect size index are also presented along with their sampling distributions (including standard errors).},
	language = {en},
	number = {4},
	urldate = {2021-02-04},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Hedges, Larry V.},
	month = dec,
	year = {2007},
	note = {Publisher: American Educational Research Association},
	keywords = {meta-analysis, cluster-randomized trials, effect size, group-randomized trials, multilevel experiments},
	pages = {341--370},
	file = {SAGE PDF Full Text:C\:\\Users\\betha\\Zotero\\storage\\97YRP72D\\Hedges - 2007 - Effect Sizes in Cluster-Randomized Designs.pdf:application/pdf},
}


@article{hedges2010,
  ids = {hedges2010a},
  title = {Robust Variance Estimation in Meta-Regression with Dependent Effect Size Estimates},
  author = {Hedges, Larry V. and Tipton, Elizabeth and Johnson, Matthew C.},
  year = {2010},
  journal = {Research Synthesis Methods},
  volume = {1},
  number = {1},
  pages = {39--65},
  issn = {1759-2887},
  doi = {10.1002/jrsm.5},
  urldate = {2021-07-16},
  abstract = {Conventional meta-analytic techniques rely on the assumption that effect size estimates from different studies are independent and have sampling distributions with known conditional variances. The independence assumption is violated when studies produce several estimates based on the same individuals or there are clusters of studies that are not independent (such as those carried out by the same investigator or laboratory). This paper provides an estimator of the covariance matrix of meta-regression coefficients that are applicable when there are clusters of internally correlated estimates. It makes no assumptions about the specific form of the sampling distributions of the effect sizes, nor does it require knowledge of the covariance structure of the dependent estimates. Moreover, this paper demonstrates that the meta-regression coefficients are consistent and asymptotically normally distributed and that the robust variance estimator is valid even when the covariates are random. The theory is asymptotic in the number of studies, but simulations suggest that the theory may yield accurate results with as few as 20--40 studies. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  pmid = {26056092},
  keywords = {dependent effects,meta analysis,meta regression,robust standard errors},
  file = {C\:\\Users\\micro\\Zotero\\storage\\V7IGQGRN\\Hedges et al. - 2010 - Robust variance estimation in meta-regression with.pdf;C\:\\Users\\micro\\Zotero\\storage\\5PVH84QC\\jrsm.html}
}

@article{higgins2002,
  title = {Quantifying Heterogeneity in a Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  year = {2002},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {11},
  pages = {1539--1558},
  issn = {0277-6715},
  doi = {10.1002/sim.1186},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.},
  langid = {english},
  pmid = {12111919},
  keywords = {{Chemotherapy, Adjuvant},Albumins,Clinical Trials as Topic,Cognition Disorders,Cytidine Diphosphate Choline,Fibrosis,Fracture Fixation,Hip Fractures,Humans,Meta-Analysis as Topic,Resuscitation,Sarcoma,Sclerotherapy,Statistics as Topic}
}

@article{higgins2009,
  title = {A Re-Evaluation of Random-Effects Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G. and Spiegelhalter, David J.},
  year = {2009},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {172},
  number = {1},
  pages = {137--159},
  publisher = {Oxford University Press / USA},
  issn = {09641998},
  doi = {10.1111/j.1467-985X.2008.00552.x},
  urldate = {2024-10-27},
  abstract = {Meta-analysis in the presence of unexplained heterogeneity is frequently undertaken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justification and interpretation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without problems, including computational intensity and sensitivity to a priori judgements. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of `set shifting' ability in people with eating disorders.},
  keywords = {Distribution (Probability theory),Heterogeneity,Hypothesis,Inference (Logic),Meta-analysis,Prediction,Psychometrics,Random-effects models,Systematic reviews},
  file = {C:\Users\micro\Zotero\storage\ATZ3T8PT\Higgins et al. - 2009 - A re-evaluation of random-effects meta-analysis.pdf}
}

  @Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }

@article{jackson2006,
  title = {The Power of the Standard Test for the Presence of Heterogeneity in Meta-Analysis},
  author = {Jackson, Dan},
  year = {2006},
  journal = {Statistics in Medicine},
  volume = {25},
  number = {15},
  pages = {2688--2699},
  issn = {1097-0258},
  doi = {10.1002/sim.2481},
  urldate = {2024-03-21},
  abstract = {It has been suggested that the standard test for the presence of heterogeneity in meta-analysis has low power. Although this has been investigated using simulation, there is little direct analytical evidence of the validity of this claim. Using an established approximate distribution for the test statistic, a procedure for obtaining the power of the test is described. From this, a simple formula for the power is obtained. Although this applies to a special case, the formula gives an indication of the power of the test more generally. In particular, for a given significance level, the power can be calibrated in terms of the proportion of the studies' variances that is provided by between-study variation. A consideration of this quantity confirms that the test does, in general, have low power. It is suggested that practitioners, who wish to conduct the standard test, use the ideas provided in order to investigate the operating characteristics of the test prior to performing it. Copyright {\copyright} 2005 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {heterogeneity,meta-analysis,standard test},
  file = {C\:\\Users\\micro\\Zotero\\storage\\P4GWD5W7\\Jackson - 2006 - The power of the standard test for the presence of.pdf;C\:\\Users\\micro\\Zotero\\storage\\HX3L8DWH\\sim.html}
}

@article{jackson2017,
  title = {Power Analysis for Random-Effects Meta-Analysis},
  author = {Jackson, Dan and Turner, Rebecca},
  year = {2017},
  journal = {Research Synthesis Methods},
  volume = {8},
  number = {3},
  pages = {290--302},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1240},
  urldate = {2023-07-17},
  abstract = {One of the reasons for the popularity of meta-analysis is the notion that these analyses will possess more power to detect effects than individual studies. This is inevitably the case under a fixed-effect model. However, the inclusion of the between-study variance in the random-effects model, and the need to estimate this parameter, can have unfortunate implications for this power. We develop methods for assessing the power of random-effects meta-analyses, and the average power of the individual studies that contribute to meta-analyses, so that these powers can be compared. In addition to deriving new analytical results and methods, we apply our methods to 1991 meta-analyses taken from the Cochrane Database of Systematic Reviews to retrospectively calculate their powers. We find that, in practice, 5 or more studies are needed to reasonably consistently achieve powers from random-effects meta-analyses that are greater than the studies that contribute to them. Not only is statistical inference under the random-effects model challenging when there are very few studies but also less worthwhile in such cases. The assumption that meta-analysis will result in an increase in power is challenged by our findings.},
  copyright = {Copyright {\copyright} 2017 The Authors. Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {cochrane,empirical evaluation,power calculations,random-effects meta-analysis},
  file = {C\:\\Users\\micro\\Zotero\\storage\\RNNJPRDC\\Jackson and Turner - 2017 - Power analysis for random-effects meta-analysis.pdf;C\:\\Users\\micro\\Zotero\\storage\\68ZXA9KH\\jrsm.html}
}

@article{jane,
  title = {Extracting {{Pre}}/{{Post Correlations}} for {{Meta-Analyses}} of {{Repeated Measures Designs}}},
  author = {Jan{\'e}, Matthew B and Harlow, Tylor J and Shah, Saachi and Veiner, Ella and Khu, Enrico and Gould, Alexander and Kuo, Yuna and Hsu, Chaney and Johnson, Blair T},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\LM29DTP3\Jané et al. - Extracting PrePost Correlations for Meta-Analyses.pdf}
}


@article{johnson2019,
  title = {Systematic Reviews and Meta-Analyses in the Health Sciences: {{Best}} Practice Methods for Research Syntheses},
  shorttitle = {Systematic Reviews and Meta-Analyses in the Health Sciences},
  author = {Johnson, Blair T. and Hennessy, Emily A.},
  year = {2019},
  month = jul,
  journal = {Social Science \& Medicine},
  volume = {233},
  pages = {237--251},
  issn = {0277-9536},
  doi = {10.1016/j.socscimed.2019.05.035},
  urldate = {2023-11-16},
  abstract = {Rationale The journal Social Science \& Medicine recently adopted the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA; Moher et al., 2009) as guidelines for authors to use when disseminating their systematic reviews (SRs). Approach After providing a brief history of evidence synthesis, this article describes why reporting standards are important, summarizes the sequential steps involved in conducting SRs and meta-analyses, and outlines additional methodological issues that researchers should address when conducting and reporting results from their SRs. Results and conclusions Successful SRs result when teams of reviewers with appropriate expertise use the highest scientific rigor in all steps of the SR process. Thus, SRs that lack foresight are unlikely to prove successful. We advocate that SR teams consider potential moderators (M) when defining their research problem, along with Time, Outcomes, Population, Intervention, Context, and Study design (i.e., TOPICS~+~M). We also show that, because the PRISMA reporting standards only partially overlap dimensions of methodological quality, it is possible for SRs to satisfy PRISMA standards yet still have poor methodological quality. As well, we discuss limitations of such standards and instruments in the face of the assumptions of the SR process, including meta-analysis spanning the other SR steps, which are highly synergistic: Study search and selection, coding of study characteristics and effects, analysis, interpretation, reporting, and finally, re-analysis and criticism. When a SR targets an important question with the best possible SR methods, its results can become a definitive statement that guides future research and policy decisions for years to come.},
  keywords = {Evidence synthesis,Meta-analysis,Methodological quality,Research synthesis,Risk of bias,Systematic reviews},
  file = {C\:\\Users\\micro\\Zotero\\storage\\YC4PCYSC\\Johnson and Hennessy - 2019 - Systematic reviews and meta-analyses in the health.pdf;C\:\\Users\\micro\\Zotero\\storage\\Z6T9VC2V\\S0277953619302990.html}
}

@Manual{tidyr,
    title = {tidyr: Tidy Messy Data},
    author = {Hadley Wickham and Davis Vaughan and Maximilian Girlich},
    year = {2024},
    note = {R package version 1.3.1},
    url = {https://CRAN.R-project.org/package=tidyr},
    doi = {10.32614/CRAN.package.tidyr},
  }

  @Manual{dplyr,
    title = {dplyr: A Grammar of Data Manipulation},
    author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
    year = {2023},
    note = {R package version 1.1.4},
    url = {https://CRAN.R-project.org/package=dplyr},
    doi = {10.32614/CRAN.package.dplyr},
  }


 @Manual{purrr,
    title = {purrr: Functional Programming Tools},
    author = {Hadley Wickham and Lionel Henry},
    year = {2025},
    note = {R package version 1.0.4},
    url = {https://CRAN.R-project.org/package=purrr},
    doi = {10.32614/CRAN.package.purrr},
  }

@BOOK{lapack99,
AUTHOR = {Anderson, E. and Bai, Z. and Bischof, C. and
Blackford, S. and Demmel, J. and Dongarra, J. and
Du Croz, J. and Greenbaum, A. and Hammarling, S. and
McKenney, A. and Sorensen, D.},
TITLE = {{LAPACK} Users' Guide},
EDITION = {Third},
PUBLISHER = {Society for Industrial and Applied Mathematics},
YEAR = {1999},
ADDRESS = {Philadelphia, PA},
ISBN = {0-89871-447-8 (paperback)} 
} 

@incollection{betsy_jane_becker_model,
	title = {Model-{Based} {Meta}-{Analysis} and {Related} {Approaches}},
	isbn = {978-1-61044-886-4},
	language = {en},
	booktitle = {The {Handbook} of {Research} {Synthesis} and {Meta}-{Analysis}},
	publisher = {Russell Sage Foundation},
	author = {{Betsy Jane Becker} and Aloe, Ariel M.},
	editor = {Cooper, Harris and Hedges, Larry V and Valentine, Jeffrey C.},
	month = jun,
	year = {2019},
	note = {Google-Books-ID: tfeXDwAAQBAJ},
	keywords = {Psychology / Experimental Psychology, Social Science / General, Social Science / Methodology, Social Science / Research},
	pages = {339--364},
}



  @Manual{stringr,
    title = {stringr: Simple, Consistent Wrappers for Common String Operations},
    author = {Hadley Wickham},
    year = {2023},
    note = {R package version 1.5.1},
    url = {https://CRAN.R-project.org/package=stringr},
    doi = {10.32614/CRAN.package.stringr},
  }

 @Book{mvtnorm,
    title = {Computation of Multivariate Normal and t Probabilities},
    author = {Alan Genz and Frank Bretz},
    series = {Lecture Notes in Statistics},
    year = {2009},
    publisher = {Springer-Verlag},
    address = {Heidelberg},
    isbn = {978-3-642-01688-2},
  }

@article{knapp2003,
  title = {{Improved tests for a random effects meta-regression with a single covariate}},
  author = {Knapp, Guido and Hartung, Joachim},
  year = {2003},
  journal = {Statistics in Medicine},
  volume = {22},
  number = {17},
  pages = {2693--2710},
  issn = {1097-0258},
  doi = {10.1002/sim.1482},
  urldate = {2024-10-27},
  abstract = {The explanation of heterogeneity plays an important role in meta-analysis. The random effects meta-regression model allows the inclusion of trial-specific covariates which may explain a part of the heterogeneity. We examine the commonly used tests on the parameters in the random effects meta-regression with one covariate and propose some new test statistics based on an improved estimator of the variance of the parameter estimates. The approximation of the distribution of the newly proposed tests is based on some theoretical considerations. Moreover, the newly proposed tests can easily be extended to the case of more than one covariate. In a simulation study, we compare the tests with regard to their actual significance level and we consider the log relative risk as the parameter of interest. Our simulation study reflects the meta-analysis of the efficacy of a vaccine for the prevention of tuberculosis originally discussed in Berkey et al. The simulation study shows that the newly proposed tests are superior to the commonly used test in holding the nominal significance level. Copyright {\copyright} 2003 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2003 John Wiley \& Sons, Ltd.},
  langid = {french},
  keywords = {heterogeneity,meta-analysis,meta-regression,trial-specific covariate},
  file = {C\:\\Users\\micro\\Zotero\\storage\\AVH2UCA5\\Knapp and Hartung - 2003 - Improved tests for a random effects meta-regressio.pdf;C\:\\Users\\micro\\Zotero\\storage\\FBQ5DML3\\sim.html}
}

@article{joshi_cluster_2022,
	title = {Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies},
	volume = {13},
	copyright = {© 2022 John Wiley \& Sons, Ltd},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1554},
	doi = {10.1002/jrsm.1554},
	abstract = {The most common and well-known meta-regression models work under the assumption that there is only one effect size estimate per study and that the estimates are independent. However, meta-analytic reviews of social science research often include multiple effect size estimates per primary study, leading to dependence in the estimates. Some meta-analyses also include multiple studies conducted by the same lab or investigator, creating another potential source of dependence. An increasingly popular method to handle dependence is robust variance estimation (RVE), but this method can result in inflated Type I error rates when the number of studies is small. Small-sample correction methods for RVE have been shown to control Type I error rates adequately but may be overly conservative, especially for tests of multiple-contrast hypotheses. We evaluated an alternative method for handling dependence, cluster wild bootstrapping, which has been examined in the econometrics literature but not in the context of meta-analysis. Results from two simulation studies indicate that cluster wild bootstrapping maintains adequate Type I error rates and provides more power than extant small-sample correction methods, particularly for multiple-contrast hypothesis tests. We recommend using cluster wild bootstrapping to conduct hypothesis tests for meta-analyses with a small number of studies. We have also created an R package that implements such tests.},
	language = {en},
	number = {4},
	urldate = {2025-05-10},
	journal = {Research Synthesis Methods},
	author = {Joshi, Megha and Pustejovsky, James E. and Beretvas, S. Natasha},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1554},
	keywords = {cluster wild bootstrap, dependence, meta-analysis, robust variance estimation},
	pages = {457--477},
	file = {Full Text PDF:C\:\\Users\\betha\\Zotero\\storage\\QHZQ9U6A\\Joshi et al. - 2022 - Cluster wild bootstrapping to handle dependent eff.pdf:application/pdf;Snapshot:C\:\\Users\\betha\\Zotero\\storage\\B7MTDBD6\\jrsm.html:text/html},
}


@article{KraftMatthewA.2020IESo,
copyright = {2020 AERA},
abstract = {Researchers commonly interpret effect sizes by applying benchmarks proposed by Jacob Cohen over a half century ago. However, effects that are small by Cohen’s standards are large relative to the impacts of most field-based interventions. These benchmarks also fail to consider important differences in study features, program costs, and scalability. In this article, I present five broad guidelines for interpreting effect sizes that are applicable across the social sciences. I then propose a more structured schema with new empirical benchmarks for interpreting a specific class of studies: causal research on education interventions with standardized achievement outcomes. Together, these tools provide a practical approach for incorporating study features, costs, and scalability into the process of interpreting the policy importance of effect sizes.},
author = {Kraft, Matthew A.},
address = {Los Angeles, CA},
issn = {0013-189X},
journal = {Educational researcher},
keywords = {Academic achievement ; Bench-marks ; Benchmarking ; Education and state ; Educational attainment ; Educational change ; Program Evaluation ; Social sciences},
language = {eng},
number = {4},
pages = {241-253},
publisher = {SAGE Publications},
title = {Interpreting Effect Sizes of Education Interventions},
volume = {49},
year = {2020},
}





@article{LindenAudreyHelen2021HoRR,
copyright = {The Author(s) 2021},
abstract = {Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.},
author = {Linden, Audrey Helen and Hönekopp, Johannes},
address = {Los Angeles, CA},
issn = {1745-6916},
journal = {Perspectives on psychological science},
keywords = {Behavioral Research ; Cognitive psychology ; Female ; Human beings ; Male ; Psychology ; Selection Bias ; Social psychology},
language = {eng},
number = {2},
pages = {358-376},
publisher = {SAGE Publications},
title = {Heterogeneity of Research Results: A New Perspective From Which to Assess and Promote Progress in Psychological Science},
volume = {16},
year = {2021},
}



@article{liu2004,
  title = {A {{Note}} on the {{Noncentrality Parameter}} and {{Effect Size Estimates}} for the {{F Test}} in {{ANOVA}}},
  author = {Liu, Xiaofeng and Raudenbush, Stephen},
  year = {2004},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {29},
  number = {2},
  eprint = {3701269},
  eprinttype = {jstor},
  pages = {251--255},
  publisher = {[American Educational Research Association, Sage Publications, Inc., American Statistical Association]},
  issn = {1076-9986},
  urldate = {2024-08-23},
  abstract = {The noncentrality parameter for the noncentral F is a precision-weighted sum of squares of treatment means, which is closely related to the test statistic and effect size. The two common effect size estimates are not based on the uniformly minimum variance unbiased (UMVU) estimate of the noncentrality parameter. The UMVU estimate of the noncentrality parameter implies a new and more conservative effect size estimate.},
  file = {C:\Users\micro\Zotero\storage\6MF4QU8V\Liu and Raudenbush - 2004 - A Note on the Noncentrality Parameter and Effect S.pdf}
}

@article{liu2020,
  title = {Comprehensive Meta-Analysis of Resilience Interventions},
  author = {Liu, Jenny J. W. and Ein, Natalie and Gervasio, Julia and Battaion, Mira and Reed, Maureen and Vickers, Kristin},
  year = {2020},
  month = dec,
  journal = {Clinical Psychology Review},
  volume = {82},
  pages = {101919},
  issn = {0272-7358},
  doi = {10.1016/j.cpr.2020.101919},
  urldate = {2023-11-16},
  abstract = {Background/Rationale There is no current consensus on operational definitions of resilience. Instead, researchers often debate the optimal approach to understanding resilience, while continuing to explore ways to enhance and/or promote its qualities in various populations. The goal of the current meta-analysis is to substantiate existing evidence examining the promotion of resilience through various interventions. Particular emphasis was placed upon the factors that contribute to variability across interventions, such as age, gender, duration of intervention, intervention approaches and risk exposure of targeted population. Method The literature search was conducted on May 28, 2019. Search terms included ``resilience intervention'' OR ``promoting resilience'' OR ``promoting resiliency'' OR ``resilience-based intervention''. A total of 268 studies, with 1584 independent samples, were included in the meta-analysis. In addition to overall efficacy, outcome-based analyses were conducted for intervention outcomes based on action, biophysical, coping, emotion, resilience, symptoms, and well-being. Finally, moderators of age, gender, length of intervention, intervention approach, intervention target, and the level of risk exposure of the sampled population were examined as moderators. Results The multi-level meta-analysis indicated that resilience-promoting interventions yielded a small, but statistically significant overall effect, Hedges's g~=~0.48 (SE~=~0.04, 95\% CI~=~[0.40, 0.56]. The variability in study effect sizes within and between studies was significant, p~{$<~$}.001, with many falling short of the threshold for practical significance. Discussion Findings lend some support for the overall efficacy of resilience interventions. However, empirical results should be cautiously interpreted in tandem with their theoretical relevance and potential advancements to the construct of resilience. Variabilities across findings reflect the current ambiguities surrounding the conceptualization and operationalization of resilience. Directions for future research on resilience as well as practical considerations are discussed.},
  keywords = {Meta-analysis,Promoting resiliency,Resilience,Resilience intervention},
  file = {C\:\\Users\\micro\\Zotero\\storage\\9VQZIERI\\Liu et al. - 2020 - Comprehensive meta-analysis of resilience interven.pdf;C\:\\Users\\micro\\Zotero\\storage\\SA8RMQ88\\S0272735820301070.html}
}

@inproceedings{mccaffrey2001,
  title = {{{GENERALIZATIONS OF BIASED REDUCED LINEARIZATION}}},
  author = {McCaffrey, D. and Bell, R. and Botts, C.},
  year = {2001},
  urldate = {2024-10-30},
  abstract = {Linearization (Skinner 1989) is a nonparametric method for estimating the standard errors of designbased statistics such as means and ratios as well as coefficients from linear and nonlinear regression models. Although the traditional linearization estimator for standard errors is consistent as the number of primary sampling units (PSUs) grows, the estimator can be biased, in particular biased low, when the number of PSUs is small or when the predictor variables are unbalanced across the PSUs (Bell and McCaffrey 2000; Kott 1994; Murray et al. 1998). Bell and McCaffrey (2000) developed biased reduced linearization (BRL) to eliminate or reduce this bias for linear regression models with unweighted data from nonstratified two-stage samples. Reduction in bias is achieved by replacing the ordinary residuals used in the standard linearization estimator by residuals adjusted to better approximate the joint distribution of the true errors. In this paper, we extend the BRL method to weighted regression analyses. The method handles a variety of different types of weights, including: {$\bullet$} design weights equal to the inverse of the sample selection probability; {$\bullet$} weights that account for post-stratification, nonresponse, and other weighting adjustments (e.g., for multiplicity) provided the weights can be treated as known; {$\bullet$} diagonal or nondiagonal precisions weights to account for heteroskedastic or correlated errors; {$\bullet$} logistic regression and other generalized linear models that can be fit by iteratively reweighted least squares; and {$\bullet$} generalized estimating equations. We discuss four alternative BRL specifications and investigate the performance (bias and variance) of these estimators and commonly used alternative via simulation. We also present an application of logistic regression used to estimate the treatment effect in a clusterrandomized experiment. The application demonstrates a natural extension of BRL to models where parameters are estimated by iteratively reweighted least squares. 2. BIAS REDUCED LINEARIZATION FOR WEIGHTED LEAST SQUARES},
  file = {C:\Users\micro\Zotero\storage\Y9MQ3PZI\McCaffrey et al. - 2001 - GENERALIZATIONS OF BIASED REDUCED LINEARIZATION.pdf}
}

@book{pigott2012,
  title = {Advances in {{Meta-Analysis}}},
  author = {Pigott, Terri D.},
  year = {2012},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4614-2278-5},
  urldate = {2024-10-31},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-1-4614-2277-8 978-1-4614-2278-5},
  langid = {english},
  keywords = {ANOVA Model,Complex Data,Meta-Analysis,Meta-Regression,Variance Component}
}
 @Manual{POMADE,
    title = {POMADE: Power for Meta-Analysis of Dependent Effects},
    author = {Mikkel H. Vembye and James E. Pustejovsky},
    year = {2024},
    note = {R package version 0.2.0},
    url = {https://CRAN.R-project.org/package=POMADE},
    doi = {10.32614/CRAN.package.POMADE},
  }

@misc{pustejovsky_wald_2025,
	type = {{clubSandwich}},
	title = {Wald tests of multiple-constraint null hypotheses},
	url = {https://jepusto.github.io/clubSandwich/articles/Wald-tests-in-clubSandwich.html},
	language = {en},
	urldate = {2025-05-18},
	author = {Pustejovsky, James E.},
	month = mar,
	year = {2025},
	file = {Snapshot:C\:\\Users\\betha\\Zotero\\storage\\6MIAMYYX\\Wald-tests-in-clubSandwich.html:text/html},
}

@book{pigott_advances_2012,
	address = {New York, NY, UNITED STATES},
	title = {Advances in {Meta}-{Analysis}},
	isbn = {978-1-4614-2278-5},
	url = {http://ebookcentral.proquest.com/lib/utxa/detail.action?docID=884387},
	abstract = {The subject of the book is advanced statistical analyses for quantitative research synthesis (meta-analysis), and selected practical issues relating to research synthesis that are not covered in detail in the many existing introductory books on research synthesis (or meta-analysis). Complex statistical issues are arising more frequently as the primary research that is summarized in quantitative syntheses itself becomes more complex, and as researchers who are conducting meta-analyses become more ambitious in the questions they wish to address. Also as researchers have gained more experience in conducting research syntheses, several key issues have persisted and now appear fundamental to the enterprise of summarizing research. Specifically the book describes multivariate analyses for several indices commonly used in meta-analysis (e.g., correlations, effect sizes, proportions and/or odds ratios), will outline how to do power analysis for meta-analysis (again for each of the different kinds of study outcome indices), and examines issues around research quality and research design and their roles in synthesis. For each of the statistical topics we will examine the different possible statistical models (i.e., fixed, random, and mixed models) that could be adopted by a researcher. In dealing with the issues of study quality and research design it covers a number of specific topics that are of broad concern to research synthesists. In many fields a current issue is how to make sense of results when studies using several different designs appear in a research literature (e.g., Morris \& Deshon, 1997, 2002). In education and other social sciences a critical aspect of this issue is how one might incorporate qualitative (e.g., case study) research within a synthesis. In medicine, related issues concern whether and how to summarize observational studies, and whether they should be combined with randomized controlled trials (or even if they should be combined at all). For each topic, included is a worked example (e.g., for the statistical analyses) and/or a detailed description of a published research synthesis that deals with the practical (non-statistical) issues covered.},
	urldate = {2025-05-20},
	publisher = {Springer New York},
	author = {Pigott, Terri},
	year = {2012},
	keywords = {Meta-analysis},
	file = {ProQuest Ebook Snapshot:C\:\\Users\\betha\\Zotero\\storage\\ISXSLKXR\\detail.html:text/html},
}


@article{fasi_computing_2023,
	title = {Computing the {Square} {Root} of a {Low}-{Rank} {Perturbation} of the {Scaled} {Identity} {Matrix}},
	volume = {44},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/10.1137/22M1471559},
	doi = {10.1137/22M1471559},
	abstract = {.This work is concerned with computing low-rank approximations of a matrix function  for a large symmetric positive semidefinite matrix , a task that arises in, e.g., statistical learning and inverse problems. The application of popular randomized methods, such as the randomized singular value decomposition or the Nyström approximation, to  requires multiplying  with a few random vectors. A significant disadvantage of such an approach, matrix-vector products with  are considerably more expensive than matrix-vector products with , even when carried out only approximately via, e.g., the Lanczos method. In this work, we present and analyze funNyström, a simple and inexpensive method that constructs a low-rank approximation of  directly from a Nyström approximation of , completely bypassing the need for matrix-vector products with . It is sensible to use funNyström whenever  is monotone and satisfies . Under the stronger assumption that  is operator monotone, which includes the matrix square root  and the matrix logarithm , we derive probabilistic bounds for the error in the Frobenius, nuclear, and operator norms. These bounds confirm the numerical observation that funNyström tends to return an approximation that compares well with the best low-rank approximation of . Furthermore, compared to existing methods, funNyström requires significantly fewer matrix-vector products with  to obtain a low-rank approximation of , without sacrificing accuracy or reliability. Our method is also of interest when estimating quantities associated with , such as the trace or the diagonal entries of . In particular, we propose and analyze funNyström++, a combination of funNyström with the recently developed Hutch++ method for trace estimation.},
	number = {1},
	urldate = {2025-05-22},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Fasi, Massimiliano and Higham, Nicholas J. and Liu, Xiaobo},
	month = mar,
	year = {2023},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {156--174},
	file = {Full Text PDF:C\:\\Users\\betha\\Zotero\\storage\\SZ4AJKPS\\Fasi et al. - 2023 - Computing the Square Root of a Low-Rank Perturbati.pdf:application/pdf},
}



@article{pigott2020,
  title = {Methodological {{Guidance Paper}}: {{High-Quality Meta-Analysis}} in a                     {{Systematic Review}}},
  shorttitle = {Methodological {{Guidance Paper}}},
  author = {Pigott, Terri D. and Polanin, Joshua R.},
  year = {2020},
  month = feb,
  journal = {Review of Educational Research},
  volume = {90},
  number = {1},
  pages = {24--46},
  publisher = {American Educational Research Association},
  address = {Los Angeles, CA},
  issn = {0034-6543},
  doi = {10.3102/0034654319877153},
  urldate = {2024-03-21},
  abstract = {This methodological guidance article discusses the elements of a high-quality meta-analysis that is conducted within the context of a systematic review. Meta-analysis, a set of statistical techniques for synthesizing the results of multiple studies, is used when the guiding research question focuses on a quantitative summary of study results. In this guidance article, we discuss the systematic review methods that support high-quality meta-analyses and outline best practice meta-analysis methods for describing the distribution of effect sizes in a set of eligible studies. We also provide suggestions for transparently reporting the methods and results of meta-analyses to influence practice and policy. Given the increasing use of meta-analysis for important policy decisions, the methods and results of meta-analysis should be both transparent and reproducible.},
  langid = {english},
  keywords = {Meta-analysis,Systematic review},
  file = {C:\Users\micro\Zotero\storage\U7RPWP4A\Pigott and Polanin - 2020 - Methodological Guidance Paper High-Quality Meta-A.pdf}
}

@article{polanin2016,
  title = {Estimating the {{Difference Between Published}} and {{Unpublished Effect Sizes}}: {{A Meta-Review}}},
  shorttitle = {Estimating the {{Difference Between Published}} and {{Unpublished Effect Sizes}}},
  author = {Polanin, Joshua R. and {Tanner-Smith}, Emily E. and Hennessy, Emily A.},
  year = {2016},
  journal = {Review of Educational Research},
  volume = {86},
  number = {1},
  eprint = {24752873},
  eprinttype = {jstor},
  pages = {207--236},
  publisher = {[Sage Publications, Inc., American Educational Research Association]},
  issn = {0034-6543},
  urldate = {2024-10-27},
  abstract = {Practitioners and policymakers rely on meta-analyses to inform decision making around the allocation of resources to individuals and organizations. It is therefore paramount to consider the validity of these results. A well-documented threat to the validity of research synthesis results is the presence of publication bias, a phenomenon where studies with large and/or statistically significant effects, relative to studies with small or null effects, are more likely to be published. We investigated this phenomenon empirically by reviewing meta-analyses published in top-tier journals between 1986 and 2013 that quantified the difference between effect sizes from published and unpublished research. We reviewed 383 meta-analyses of which 81 had sufficient information to calculate an effect size. Results indicated that published studies yielded larger effect sizes than those from unpublished studies (d; = 0.18, 95\% confidence interval [0.10, 0.25]). Moderator analyses revealed that the difference was larger in meta-analyses that included a wide range of unpublished literature. We conclude that intervention researchers require continued support to publish null findings and that meta-analyses should include unpublished studies to mitigate the potential bias from publication status.},
  file = {C:\Users\micro\Zotero\storage\AZB6LTK2\Polanin et al. - 2016 - Estimating the Difference Between Published and Un.pdf}
}

@article{pustejovsky2018,
  ids = {pustejovsky2018a},
  title = {Small Sample Methods for Cluster-Robust Variance Estimation and Hypothesis Testing in Fixed Effects Models},
  author = {Pustejovsky, James E. and Tipton, Elizabeth},
  year = {2018},
  month = oct,
  journal = {Journal of Business \& Economic Statistics},
  volume = {36},
  number = {4},
  eprint = {1601.01981},
  primaryclass = {stat},
  pages = {672--683},
  publisher = {Taylor \& Francis},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2016.1247004},
  urldate = {2024-10-28},
  abstract = {In panel data models and other regressions with unobserved effects, fixed effects estimation is often paired with cluster-robust variance estimation (CRVE) in order to account for heteroskedasticity and un-modeled dependence among the errors. Although asymptotically consistent, CRVE can be biased downward when the number of clusters is small, leading to hypothesis tests with rejection rates that are too high. More accurate tests can be constructed using bias-reduced linearization (BRL), which corrects the CRVE based on a working model, in conjunction with a Satterthwaite approximation for t-tests. We propose a generalization of BRL that can be applied in models with arbitrary sets of fixed effects, where the original BRL method is undefined, and describe how to apply the method when the regression is estimated after absorbing the fixed effects. We also propose a small-sample test for multiple-parameter hypotheses, which generalizes the Satterthwaite approximation for t-tests. In simulations covering a wide range of scenarios, we find that the conventional cluster-robust Wald test can severely over-reject while the proposed small-sample test maintains Type I error close to nominal levels. The proposed methods are implemented in an R package called clubSandwich.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Cluster dependence,Fixed effects,Robust standard errors,Small samples,Statistics - Methodology},
  file = {C\:\\Users\\micro\\Zotero\\storage\\E6RZPBRN\\Pustejovsky and Tipton - 2018 - Small-Sample Methods for Cluster-Robust Variance E.pdf;C\:\\Users\\micro\\Zotero\\storage\\RF4EKS23\\1601.pdf}
}

@article{pustejovsky2022,
  title = {Meta-Analysis with {{Robust Variance Estimation}}: {{Expanding}} the {{Range}} of {{Working Models}}},
  shorttitle = {Meta-Analysis with {{Robust Variance Estimation}}},
  author = {Pustejovsky, James E.  and Tipton, Elizabeth},
  year = {2022},
  month = apr,
  journal = {Prevention Science},
  volume = {23},
  number = {3},
  pages = {425--438},
  publisher = {Springer Nature B.V.},
  address = {New York, Netherlands},
  issn = {13894986},
  doi = {10.1007/s11121-021-01246-3},
  urldate = {2023-07-17},
  abstract = {In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the exact form of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multilevel and multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer potential benefits in terms of better capturing the types of data structures that occur in practice and, under some circumstances, improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the ``metafor'' and ``clubSandwich'' packages for R), illustrate the proposed approach in a meta-analysis of randomized trials on the effects of brief alcohol interventions for adolescents and young adults, and report findings from a simulation study evaluating the performance of the new methods.},
  copyright = {{\copyright} Society for Prevention Research 2021.},
  langid = {english},
  keywords = {Dependent effect sizes,Meta-analysis,Meta-regression,Robust standard errors},
  file = {C:\Users\micro\Zotero\storage\AKJ3T5Q9\Pustejovsky et al. - 2022 - Meta-analysis with Robust Variance Estimation Exp.pdf}
}

@misc{pustejovsky2024,
  title = {Approximating the Distribution of Cluster-Robust {{Wald}} Statistics},
  author = {Pustejovsky, James E.},
  year = {2024},
  month = mar,
  urldate = {2024-10-24},
  abstract = {Education Statistics and Meta-Analysis},
  url = {https://jepusto.netlify.app/posts/cluster-robust-Wald-statistics/},
  langid = {english}
}


@Manual{pustejovsky2024a,
    title = {clubSandwich: Cluster-Robust (Sandwich) Variance Estimators with Small-Sample
Corrections},
    author = {James E. Pustejovsky},
    year = {2025},
    note = {R package version 0.6.0},
    url = {https://CRAN.R-project.org/package=clubSandwich},
    doi = {10.32614/CRAN.package.clubSandwich},
  }



@article{quintana2023,
  title = {A {{Guide}} for {{Calculating Study-Level Statistical Power}} for {{Meta-Analyses}}},
  author = {Quintana, Daniel S.},
  year = {2023},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {6},
  number = {1},
  pages = {25152459221147260},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459221147260},
  urldate = {2024-03-21},
  abstract = {Meta-analysis is a popular approach in the psychological sciences for synthesizing data across studies. However, the credibility of meta-analysis outcomes depends on the evidential value of studies included in the body of evidence used for data synthesis. One important consideration for determining a study's evidential value is the statistical power of the study's design/statistical test combination for detecting hypothetical effect sizes of interest. Studies with a design/test combination that cannot reliably detect a wide range of effect sizes are more susceptible to questionable research practices and exaggerated effect sizes. Therefore, determining the statistical power for design/test combinations for studies included in meta-analyses can help researchers make decisions regarding confidence in the body of evidence. Because the one true population effect size is unknown when hypothesis testing, an alternative approach is to determine statistical power for a range of hypothetical effect sizes. This tutorial introduces the metameta R package and web app, which facilitates the straightforward calculation and visualization of study-level statistical power in meta-analyses for a range of hypothetical effect sizes. Readers will be shown how to reanalyze data using information typically presented in meta-analysis forest plots or tables and how to integrate the metameta package when reporting novel meta-analyses. A step-by-step companion screencast video tutorial is also provided to assist readers using the R package.},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\2Y99S8GY\Quintana - 2023 - A Guide for Calculating Study-Level Statistical Po.pdf}
}

@article{WilliamsRyan2022HiMI,
abstract = {Since the standards-based education movement began in the early 1990s, mathematics education reformers have developed and evaluated many interventions to support students in mastering more rigorous content. We conducted a systematic review and meta-analysis of U.S. PreK-12 mathematics intervention effects from 1991 to 2017 to study sources of heterogeneity. From more than 9,000 published and unpublished study reports, we found 191 randomized control trials that met our inclusion criteria, with 1,109 effect size estimates representing more than a quarter of a million students. The average effect size on student mathematics achievement was 0.31, with wide heterogeneity of most effects ranging from −0.60 to 1.23. Two modeling approaches-meta-regression and machine learning-provided converging evidence that outcome measure type (researcher-created vs. standardized) and technology delivery (vs. teacher or interventionist delivery) were predictors of effect size. Intervention type, intervention length, grade level, and publication year were also identified as potentially explanatory factors.},
author = {Williams, Ryan and Citkowicz, Martyna and Miller, David I. and Lindsay, Jim and Walters, Kirk},
address = {Philadelphia},
copyright = {2022 Taylor & Francis Group, LLC 2022},
issn = {1934-5747},
journal = {Journal of research on educational effectiveness},
keywords = {Education Preschool ; Educational change},
language = {eng},
number = {3},
pages = {584-634},
publisher = {Routledge},
title = {Heterogeneity in Mathematics Intervention Effects: Evidence from a Meta-Analysis of 191 Randomized Experiments},
volume = {15},
year = {2022},
}

@article{sera2019,
  title = {An Extended Mixed-Effects Framework for Meta-Analysis},
  author = {Sera, Francesco and Armstrong, Benedict and Blangiardo, Marta and Gasparrini, Antonio},
  year = {2019},
  journal = {Statistics in Medicine},
  volume = {38},
  number = {29},
  pages = {5429--5444},
  issn = {1097-0258},
  doi = {10.1002/sim.8362},
  urldate = {2024-10-27},
  abstract = {Standard methods for meta-analysis are limited to pooling tasks in which a single effect size is estimated from a set of independent studies. However, this setting can be too restrictive for modern meta-analytical applications. In this contribution, we illustrate a general framework for meta-analysis based on linear mixed-effects models, where potentially complex patterns of effect sizes are modeled through an extended and flexible structure of fixed and random terms. This definition includes, as special cases, a variety of meta-analytical models that have been separately proposed in the literature, such as multivariate, network, multilevel, dose-response, and longitudinal meta-analysis and meta-regression. The availability of a unified framework for meta-analysis, complemented with the implementation in a freely available and fully documented software, will provide researchers with a flexible tool for addressing nonstandard pooling problems.},
  copyright = {{\copyright} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {dose-response,longitudinal,meta-analysis,mixed-effects models},
  file = {C\:\\Users\\micro\\Zotero\\storage\\4MNKV84W\\Sera et al. - 2019 - An extended mixed-effects framework for meta-analy.pdf;C\:\\Users\\micro\\Zotero\\storage\\9Q7LIN9Q\\sim.html}
}

@article{tipton2015a,
  title = {Small Sample Adjustments for Robust Variance Estimation with Meta-Regression},
  author = {Tipton, Elizabeth},
  year = {2015},
  month = sep,
  journal = {Psychological Methods},
  volume = {20},
  number = {3},
  pages = {375--393},
  issn = {1939-1463},
  doi = {10.1037/met0000011},
  abstract = {Although primary studies often report multiple outcomes, the covariances between these outcomes are rarely reported. This leads to difficulties when combining studies in a meta-analysis. This problem was recently addressed with the introduction of robust variance estimation. This new method enables the estimation of meta-regression models with dependent effect sizes, even when the dependence structure is unknown. Although robust variance estimation has been shown to perform well when the number of studies in the meta-analysis is large, previous simulation studies suggest that the associated tests often have Type I error rates that are much larger than nominal. In this article, I introduce 6 estimators with better small sample properties and study the effectiveness of these estimators via 2 simulation studies. The results of these simulations suggest that the best estimator involves correcting both the residuals and degrees of freedom used in the robust variance estimator. These studies also suggest that the degrees of freedom depend on not only the number of studies but also the type of covariates in the meta-regression. The fact that the degrees of freedom can be small, even when the number of studies is large, suggests that these small-sample corrections should be used more generally. I conclude with an example comparing the results of a meta-regression with robust variance estimation with the results from the corrected estimator.},
  langid = {english},
  pmid = {24773356},
  keywords = {{Data Interpretation, Statistical},Humans,Meta-Analysis as Topic,Regression Analysis}
}

@article{tipton2015b,
  title = {Small-{{Sample Adjustments}} for {{Tests}} of {{Moderators}} and {{Model Fit Using Robust Variance Estimation}} in {{Meta-Regression}}},
  author = {Tipton, Elizabeth and Pustejovsky, James E.},
  year = {2015},
  month = dec,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {40},
  number = {6},
  pages = {604--634},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/1076998615606099},
  urldate = {2023-11-21},
  abstract = {Meta-analyses often include studies that report multiple effect sizes based on a common pool of subjects or that report effect sizes from several samples that were treated with very similar research protocols. The inclusion of such studies introduces dependence among the effect size estimates. When the number of studies is large, robust variance estimation (RVE) provides a method for pooling dependent effects, even when information on the exact dependence structure is not available. When the number of studies is small or moderate, however, test statistics and confidence intervals based on RVE can have inflated Type I error. This article describes and investigates several small-sample adjustments to F-statistics based on RVE. Simulation results demonstrate that one such test, which approximates the test statistic using Hotelling's T2 distribution, is level-{$\alpha$} and uniformly more powerful than the others. An empirical application demonstrates how results based on this test compare to the large-sample F-test.},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\UN8HHY26\Tipton and Pustejovsky - 2015 - Small-Sample Adjustments for Tests of Moderators a.pdf}
}

@article{tipton2019,
  title = {Current Practices in Meta-Regression in Psychology, Education, and Medicine},
  author = {Tipton, Elizabeth and Pustejovsky, James E. and Ahmadi, Hedyeh},
  year = {2019},
  journal = {Research Synthesis Methods},
  volume = {10},
  number = {2},
  pages = {180--194},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1339},
  urldate = {2023-07-17},
  abstract = {Having surveyed the history and methods of meta-regression in a previous paper, in this paper, we review which and how meta-regression methods are applied in recent research syntheses. To do so, we reviewed studies published in 2016 across four leading research synthesis journals: Psychological Bulletin, the Journal of Applied Psychology, Review of Educational Research, and the Cochrane Library. We find that the best practices defined in the previous review are rarely carried out in practice. In light of the identified discrepancies, we consider how to move forward, first by identifying areas where further methods development is needed to address persistent problems in the field and second by discussing how to more effectively disseminate points of methodological consensus.},
  copyright = {{\copyright} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {education,medicine,meta-regression,moderators,practice,psychology},
  file = {C\:\\Users\\micro\\Zotero\\storage\\MYJYZCTK\\Tipton et al. - 2019 - Current practices in meta-regression in psychology.pdf;C\:\\Users\\micro\\Zotero\\storage\\PK7UIYVQ\\jrsm.html}
}

@article{tipton2019a,
  title = {A History of Meta-regression: {{Technical}}, Conceptual, and Practical Developments between 1974 and 2018},
  shorttitle = {A History of Meta-regression},
  author = {Tipton, Elizabeth and Pustejovsky, James E. and Ahmadi, Hedyeh},
  year = {2019},
  month = jun,
  journal = {Research Synthesis Methods},
  volume = {10},
  number = {2},
  pages = {161--179},
  issn = {1759-2879, 1759-2887},
  doi = {10.1002/jrsm.1338},
  urldate = {2023-11-16},
  abstract = {At the beginning of the development of meta-analysis, understanding the role of moderators was given the highest priority, with meta-regression provided as a method for achieving this goal. Yet in current practice, meta-regression is not as commonly used as anticipated. This paper seeks to understand this mismatch by reviewing the history of meta-regression methods over the past 40~years. We divide this time span into four periods and examine three types of methodological developments within each period: technical, conceptual, and practical. Our focus is broad and includes development of methods in the fields of education, psychology, and medicine. We conclude the paper with a discussion of five consensus points, as well as open questions and areas of research for the future.},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\J4XHMAWS\Tipton et al. - 2019 - A history of meta‐regression Technical, conceptua.pdf}
}

@article{tipton2023,
  title = {Why Meta-Analyses of Growth Mindset and Other Interventions Should Follow Best Practices for Examining Heterogeneity: {{Commentary}} on {{Macnamara}} and {{Burgoyne}} (2023) and {{Burnette}} et al. (2023)},
  shorttitle = {Why Meta-Analyses of Growth Mindset and Other Interventions Should Follow Best Practices for Examining Heterogeneity},
  author = {Tipton, Elizabeth and Bryan, Christopher and Murray, Jared and McDaniel, Mark A. and Schneider, Barbara and Yeager, David S.},
  year = {2023},
  journal = {Psychological Bulletin},
  volume = {149},
  number = {3-4},
  pages = {229--241},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/bul0000384},
  abstract = {Meta-analysts often ask a yes-or-no question: Is there an intervention effect or not? This traditional, all-or-nothing thinking stands in contrast with current best practice in meta-analysis, which calls for a heterogeneity-attuned approach (i.e., focused on the extent to which effects vary across procedures, participant groups, or contexts). This heterogeneity-attuned approach allows researchers to understand where effects are weaker or stronger and reveals mechanisms. The current article builds on a rare opportunity to compare two recent meta-analyses that examined the same literature (growth mindset interventions) but used different methods and reached different conclusions. One meta-analysis used a traditional approach (Macnamara \& Burgoyne, 2023), which aggregated effect sizes for each study before combining them and examined moderators one-by-one by splitting the data into small subgroups. The second meta-analysis (Burnette et al., 2023) modeled the variation of effects within studies---across subgroups and outcomes---and applied modern, multilevel metaregression methods. The former concluded that growth mindset effects are biased, but the latter yielded nuanced conclusions consistent with theoretical predictions. We explain why the practices followed by the latter meta-analysis were more in line with best practices for analyzing large and heterogeneous literatures. Further, an exploratory re-analysis of the data showed that applying the modern, heterogeneity-attuned methods from Burnette et al. (2023) to the data set employed by Macnamara and Burgoyne (2023) confirmed Burnette et al.'s conclusions; namely, that there was a meaningful, significant effect of growth mindset in focal (at-risk) groups. This article concludes that heterogeneity-attuned meta-analysis is important both for advancing theory and for avoiding the boom-or-bust cycle that plagues too much of psychological science. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Behavioral Sciences,Best Practices,Homogeneity of Variance,Intervention,Mindset,Research Quality},
  file = {C:\Users\micro\Zotero\storage\BT6NV2RV\2023-90931-003.html}
}

@book{valentine2009,
  title = {The {{Handbook}} of {{Research Synthesis}} and {{Meta-Analysis}}},
  author = {Valentine, Jeffrey C. and Hedges, Larry V. and Cooper, Harris},
  year = {2009},
  publisher = {Russell Sage Foundation},
  address = {New York},
  urldate = {2024-08-23},
  abstract = {Praise for the first edition: "The Handbook is a comprehensive treatment of literature synthesis and provides practical advice for anyone deep in the throes of, just teetering on the brink of, or attempting to decipher a meta-analysis. Given the expanding application and importance of literature synthesis, understanding both its strengths and weaknesses is essential for its practitioners and consumers. This volume is a good beginning for those who wish to gain that understanding." ---Chance "Meta-analysis, as the statistical analysis of a large collection of results from individual studies is called, has now achieved a status of respectability in medicine. This respectability, when combined with the slight hint of mystique that sometimes surrounds meta-analysis, ensures that results of studies that use it are treated with the respect they deserve{\dots}.The Handbook of Research Synthesis is one of the most important publications in this subject both as a definitive reference book and a practical manual."---British Medical Journal When the first edition of The Handbook of Research Synthesis was published in 1994, it quickly became the definitive reference for researchers conducting meta-analyses of existing research in both the social and biological sciences. In this fully revised second edition, editors Harris Cooper, Larry Hedges, and Jeff Valentine present updated versions of the Handbook's classic chapters, as well as entirely new sections reporting on the most recent, cutting-edge developments in the field. Research synthesis is the practice of systematically distilling and integrating data from a variety of sources in order to draw more reliable conclusions about a given question or topic. The Handbook of Research Synthesis and Meta-Analysis draws upon years of groundbreaking advances that have transformed research synthesis from a narrative craft into an important scientific process in its own right. Cooper, Hedges, and Valentine have assembled leading authorities in the field to guide the reader through every stage of the research synthesis process---problem formulation, literature search and evaluation, statistical integration, and report preparation. The Handbook of Research Synthesis and Meta-Analysis incorporates state-of-the-art techniques from all quantitative synthesis traditions. Distilling a vast technical literature and many informal sources, the Handbook provides a portfolio of the most effective solutions to the problems of quantitative data integration. Among the statistical issues addressed by the authors are the synthesis of non-independent data sets, fixed and random effects methods, the performance of sensitivity analyses and model assessments, and the problem of missing data. The Handbook of Research Synthesis and Meta-Analysis also provides a rich treatment of the non-statistical aspects of research synthesis. Topics include searching the literature, and developing schemes for gathering information from study reports. Those engaged in research synthesis will also find useful advice on how tables, graphs, and narration can be used to provide the most meaningful communication of the results of research synthesis. In addition, the editors address the potentials and limitations of research synthesis, and its future directions. The past decade has been a period of enormous growth in the field of research synthesis. The second edition Handbook thoroughly revises original chapters to assure that the volume remains the most authoritative source of information for researchers undertaking meta-analysis today. In response to the increasing use of research synthesis in the formation of public policy, the second edition includes a new chapter on both the strengths and limitations of research synthesis in policy debates},
  isbn = {978-1-61044-138-4},
  file = {C\:\\Users\\micro\\Zotero\\storage\\2QX4MX7Z\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\36RY5SIP\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\4BU3JVFY\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\5ZANIW86\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\7Z9J4JFV\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\84VD6CDA\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\9IJHWW66\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\B4B279XC\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\CMPLZXPS\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\DQBLLK7S\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\EGXG8NUE\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\G3SL5K4J\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\G3WW7NUJ\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\HNPI3HDW\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\HR32WDDQ\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\IFL3GNQI\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\JPM7D56X\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\JT7K5EVU\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\L3FXDZM3\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\MFVM4DNM\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\MVCFNXSU\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\NA8HDNK4\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\NARBWKKR\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\NYWL2BG5\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\PHIBR6DK\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\QCMZHUT5\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\QYGFYW3U\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\RUXLKSMJ\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\RY29YI6F\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\S6PHMRIX\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\SJSDHMPK\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\T8QAMRTA\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\VKR4JGTQ\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\VVEGMV5D\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\WZMTUUPC\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\XNKVZQ24\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\YA5IA3UD\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf;C\:\\Users\\micro\\Zotero\\storage\\YMU5UJU2\\Valentine et al. - 2009 - The Handbook of Research Synthesis and Meta-Analys.pdf}
}

@article{valentine2010,
  title = {How {{Many Studies Do You Need}}?: {{A Primer}} on {{Statistical Power}} for {{Meta-Analysis}}},
  shorttitle = {How {{Many Studies Do You Need}}?},
  author = {Valentine, Jeffrey C. and Pigott, Therese D. and Rothstein, Hannah R.},
  year = {2010},
  month = apr,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {35},
  number = {2},
  pages = {215--247},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/1076998609346961},
  urldate = {2023-07-17},
  abstract = {In this article, the authors outline methods for using fixed and random effects power analysis in the context of meta-analysis. Like statistical power analysis for primary studies, power analysis for meta-analysis can be done either prospectively or retrospectively and requires assumptions about parameters that are unknown. The authors provide some suggestions for thinking about these parameters, in particular for the random effects variance component. The authors also show how the typically uninformative retrospective power analysis can be made more informative. The authors then discuss the value of confidence intervals, show how they could be used in addition to or instead of retrospective power analysis, and also demonstrate that confidence intervals can convey information more effectively in some situations than power analyses alone. Finally, the authors take up the question ``How many studies do you need to do a meta-analysis?'' and show that, given the need for a conclusion, the answer is ``two studies,'' because all other synthesis techniques are less transparent and/or are less likely to be valid. For systematic reviewers who choose not to conduct a quantitative synthesis, the authors provide suggestions for both highlighting the current limitations in the research base and for displaying the characteristics and results of studies that were found to meet inclusion criteria.},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\F6YMSQEU\40785162.pdf}
}

@article{vandennoortgate2013,
  ids = {vandennoortgate2013a},
  title = {Three-Level Meta-Analysis of Dependent Effect Sizes},
  author = {{Van den Noortgate}, Wim and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and {S{\'a}nchez-Meca}, Julio},
  year = {2013},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {2},
  pages = {576--594},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0261-6},
  abstract = {Although dependence in effect sizes is ubiquitous, commonly used meta-analytic methods assume independent effect sizes. We describe and illustrate three-level extensions of a mixed effects meta-analytic model that accounts for various sources of dependence within and across studies, because multilevel extensions of meta-analytic models still are not well known. We also present a three-level model for the common case where, within studies, multiple effect sizes are calculated using the same sample. Whereas this approach is relatively simple and does not require imputing values for the unknown sampling covariances, it has hardly been used, and its performance has not been empirically investigated. Therefore, we set up a simulation study, showing that also in this situation, a three-level approach yields valid results: Estimates of the treatment effects and the corresponding standard errors are unbiased.},
  langid = {english},
  pmid = {23055166},
  keywords = {{Models, Psychological},{Models, Statistical},{Models, Theoretical},Dependence,Meta-analysis,Meta-Analysis as Topic,Multilevel,Multiple outcomes,Multivariate Analysis,Sampling Studies},
  file = {C:\Users\micro\Zotero\storage\BUL2KSGW\Van den Noortgate et al. - 2013 - Three-level meta-analysis of dependent effect size.pdf}
}

@article{vandennoortgate2015,
  title = {Meta-Analysis of Multiple Outcomes: A Multilevel Approach},
  shorttitle = {Meta-Analysis of Multiple Outcomes},
  author = {{Van den Noortgate}, Wim and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and {S{\'a}nchez-Meca}, Julio},
  year = {2015},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {47},
  number = {4},
  pages = {1274--1294},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0527-2},
  urldate = {2024-08-23},
  abstract = {In meta-analysis, dependent effect sizes are very common. An example is where in one or more studies the effect of an intervention is evaluated on multiple outcome variables for the same sample of participants. In this paper, we evaluate a three-level meta-analytic model to account for this kind of dependence, extending the simulation results of Van den Noortgate, L{\'o}pez-L{\'o}pez, Mar{\'i}n-Mart{\'i}nez, and S{\'a}nchez-Meca Behavior Research Methods, 45, 576--594 (2013) by allowing for a variation in the number of effect sizes per study, in the between-study variance, in the correlations between pairs of outcomes, and in the sample size of the studies. At the same time, we explore the performance of the approach if the outcomes used in a study can be regarded as a random sample from a population of outcomes. We conclude that although this approach is relatively simple and does not require prior estimates of the sampling covariances between effect sizes, it gives appropriate mean effect size estimates, standard error estimates, and confidence interval coverage proportions in a variety of realistic situations.},
  langid = {english},
  keywords = {Dependence,Meta-analysis,Multilevel,Multiple outcomes},
  file = {C:\Users\micro\Zotero\storage\SUZZNSZV\Van den Noortgate et al. - 2015 - Meta-analysis of multiple outcomes a multilevel a.pdf}
}

@article{vembye2023,
  title = {Power {{Approximations}} for {{Overall Average Effects}} in {{Meta-Analysis With Dependent Effect Sizes}}},
  author = {Vembye, Mikkel Helding and Pustejovsky, James Eric and Pigott, Therese Deocampo},
  year = {2023},
  month = feb,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {48},
  number = {1},
  pages = {70--102},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/10769986221127379},
  urldate = {2023-07-12},
  abstract = {Meta-analytic models for dependent effect sizes have grown increasingly sophisticated over the last few decades, which has created challenges for a priori power calculations. We introduce power approximations for tests of average effect sizes based upon several common approaches for handling dependent effect sizes. In a Monte Carlo simulation, we show that the new power formulas can accurately approximate the true power of meta-analytic models for dependent effect sizes. Lastly, we investigate the Type I error rate and power for several common models, finding that tests using robust variance estimation provide better Type I error calibration than tests with model-based variance estimation. We consider implications for practice with respect to selecting a working model and an inferential approach.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\J2MHNWG6\\Vembye et al. - 2023 - Power Approximations for Overall Average Effects i.pdf;C\:\\Users\\micro\\Zotero\\storage\\XY6TNQ5F\\Supplementary Material for Power Approximations for Overall Average Effects in Meta-Analysis with Dependent Effect Sizes.pdf}
}

@misc{vembye2024,
  title = {Conducting {{Power Analysis}} for {{Meta-Analysis}} of {{Dependent Effect Sizes}}: {{Common Guidelines}} and an {{Introduction}} to the {{POMADE R}} Package},
  shorttitle = {Conducting {{Power Analysis}} for {{Meta-Analysis}} of {{Dependent Effect Sizes}}},
  author = {Vembye, Mikkel Helding and Pustejovsky, James E. and Pigott, Terri},
  year = {2024},
  month = feb,
  publisher = {OSF},
  doi = {10.31222/osf.io/3x2en},
  urldate = {2024-08-23},
  abstract = {Sample size and statistical power are important factors to consider when planning a research synthesis. Power analysis methods have been developed for fixed effect or random effects models, but until recently these methods were limited to simple data structures with a single, independent effect per study. Recent work has provided power approximation formulas for meta-analyses involving studies with multiple, dependent effect size estimates, which are common in syntheses of social science research. Prior work focused on developing and validating the approximations but did not address the practice challenges encountered in applying them for purposes of planning a synthesis involving dependent effect sizes. We aim to facilitate the application of these recent developments by providing practical guidance on how to conduct power analysis for planning a meta-analysis of dependent effect sizes and by introducing a new R package, POMADE, designed for this purpose. We present a comprehensive overview of resources for finding information about the study design features and model parameters needed to conduct power analysis, along with detailed worked examples using the POMADE package. For presenting power analysis findings, we emphasize graphical tools that can depict power under a range of plausible as-sumptions and introduce a novel plot, the traffic light power plot, for conveying the degree of certainty in one's assumptions.},
  archiveprefix = {OSF},
  langid = {american},
  file = {C:\Users\micro\Zotero\storage\W3P3JC97\Vembye et al. - 2024 - Conducting Power Analysis for Meta-Analysis of Dep.pdf}
}

@book{venables2003,
  title = {Modern {{Applied Statistics}} with {{S}}},
  author = {Venables, W. N. and Ripley, B. D.},
  year = {2003},
  month = sep,
  publisher = {Springer Science \& Business Media},
  abstract = {S-PLUS is a powerful environment for the statistical and graphical analysis of data. It provides the tools to implement many statistical ideas which have been made possible by the widespread availability of workstations having good graphics and computational capabilities. This book is a guide to using S-PLUS to perform statistical analyses and provides both an introduction to the use of S-PLUS and a course in modern statistical methods. S-PLUS is available for both Windows and UNIX workstations, and both versions are covered in depth. The aim of the book is to show how to use S-PLUS as a powerful and graphical data analysis system. Readers are assumed to have a basic grounding in statistics, and so the book in intended for would-be users of S-PLUS and both students and researchers using statistics. Throughout, the emphasis is on presenting practical problems and full analyses of real data sets. Many of the methods discussed are state-of-the-art approaches to topics such as linear, nonlinear, and smooth regression models, tree-based methods, multivariate analysis and pattern recognition, survival analysis, time series and spatial statistics. Throughout, modern techniques such as robust methods, non-parametric smoothing, and bootstrapping are used where appropriate. This third edition is intended for users of S-PLUS 4.5, 5.0, 2000 or later, although S-PLUS 3.3/4 are also considered. The major change from the second edition is coverage of the current versions of S-PLUS. The material has been extensively rewritten using new examples and the latest computationally intensive methods. The companion volume on S Programming will provide an in-depth guide for those writing software in the S language. The authors have written several software libraries that enhance S-PLUS; these and all the datasets used are available on the Internet in versions for Windows and UNIX. There are extensive on-line complements covering advanced material, user-contributed extensions, further exercises, and new features of S-PLUS as they are introduced. Dr. Venables is now Statistician with CSRIO in Queensland, having been at the Department of Statistics, University of Adelaide, for many years previously. He has given many short courses on S-PLUS in Australia, Europe, and the USA. Professor Ripley holds the Chair of Applied Statistics at the University of Oxford, and is the author of four other books on spatial statistics, simulation, pattern recognition, and neural networks.},
  googlebooks = {974c4vKurNkC},
  isbn = {978-0-387-95457-8},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / Counting \& Numeration,Mathematics / Numerical Analysis,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{viechtbauer2005,
  title = {Bias and {{Efficiency}} of {{Meta-Analytic Variance Estimators}} in the {{Random-Effects Model}}},
  author = {Viechtbauer, Wolfgang},
  year = 2005,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {30},
  number = {3},
  pages = {261--293},
  publisher = {American Educational Research Association},
  address = {Washington, United States},
  issn = {10769986},
  urldate = {2024-10-28},
  abstract = {The meta-analytic random effects model assumes that the variability in effect size estimates drawn from a set of studies can be decomposed into two parts: heterogeneity due to random population effects and sampling variance. In this context, the usual goal is to estimate the central tendency and the amount of heterogeneity in the population effect sizes. The amount of heterogeneity in a set of effect sizes has implications regarding the interpretation of the meta-analytic findings and often serves as an indicator for the presence of potential moderator variables. Five population heterogeneity estimators were compared in this article analytically and via Monte Carlo simulations with respect to their bias and efficiency. [PUBLICATION ABSTRACT]},
  copyright = {Copyright American Educational Research Association Fall 2005},
  langid = {english},
  keywords = {Bias,Effect size,Efficiency,Estimation bias,Mathematical models,Meta-analysis,Monte Carlo simulation,Random effects model,Variance analysis},
  file = {C:\Users\micro\Zotero\storage\XF47867H\Viechtbauer - 2005 - Bias and Efficiency of Meta-Analytic Variance Esti.pdf}
}

@article{viechtbauer2007,
  title = {Accounting for Heterogeneity via Random-Effects Models and Moderator Analyses in Meta-Analysis},
  author = {Viechtbauer, Wolfgang},
  year = {2007},
  journal = {Zeitschrift f{\"u}r Psychologie/Journal of Psychology},
  series = {The {{State}} and the {{Art}} of {{Meta-Analysis}}},
  volume = {215},
  number = {2},
  pages = {104--121},
  publisher = {Hogrefe \& Huber Publishers},
  issn = {0044-3409},
  doi = {10.1027/0044-3409.215.2.104},
  urldate = {2024-10-27},
  abstract = {To conduct a meta-analysis, one needs to express the results from a set of related studies in terms of an outcome measure, such as a standardized mean difference, correlation coefficient, or odds ratio. The observed outcome from a single study will differ from the true value of the outcome measure because of sampling variability. The observed outcomes from a set of related studies measuring the same outcome will, therefore, not coincide. However, one often finds that the observed outcomes differ more from each other than would be expected based on sampling variability alone. A likely explanation for this phenomenon is that the true values of the outcome measure are heterogeneous. One way to account for the heterogeneity is to assume that the heterogeneity is entirely random. Another approach is to examine whether the heterogeneity in the outcomes can be accounted for, at least in part, by a set of study-level variables describing the methods, procedures, and samples used in the different studies. The purpose of the present paper is to discuss these different approaches with particular emphasis on the interpretation of the results and practical issues. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {heterogeneous values,Homogeneity of Variance,Meta Analysis,meta-analysis,meta-regression,Models,moderator analysis,outcome heterogeneity,random-effects model,Statistical Regression,Statistical Variables,study-level variables,Treatment Effectiveness Evaluation,Treatment Outcomes},
  file = {C:\Users\micro\Zotero\storage\TTIZGTRZ\Viechtbauer - 2007 - Accounting for heterogeneity via random-effects mo.pdf}
}

@article{viechtbauer2010a,
  title = {Conducting {{Meta-Analyses}} in {{R}} with the Metafor {{Package}}},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {36},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v036.i03},
  urldate = {2024-10-27},
  abstract = {The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way.  Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.},
  copyright = {Copyright (c) 2009 Wolfgang Viechtbauer},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\NRNKQBLZ\Viechtbauer - 2010 - Conducting Meta-Analyses in R with the metafor Pac.pdf}
}

@misc{viechtbauer_convergence_2022,
	title = {Convergence {Problems} with the rma.mv() {Function}},
	url = {https://www.metafor-project.org/doku.php/tips:convergence_problems_rma_mv},
	urldate = {2025-05-16},
	journal = {The metafor Package},
	author = {Viechtbauer, Wolfgang},
	month = oct,
	year = {2022},
	file = {Convergence Problems with the rma.mv() Function [The metafor Package]:C\:\\Users\\betha\\Zotero\\storage\\PFH94Y8Q\\tipsconvergence_problems_rma_mv.html:text/html},
}



@article{viechtbauer2015,
  title = {A Comparison of Procedures to Test for Moderators in Mixed-Effects Meta-Regression Models},
  author = {Viechtbauer, Wolfgang and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio},
  year = {2015},
  month = sep,
  journal = {Psychological Methods},
  series = {Meta-{{Analysis Topics}}},
  volume = {20},
  number = {3},
  pages = {360--374},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10.1037/met0000023},
  urldate = {2021-09-12},
  abstract = {Several alternative methods are available when testing for moderators in mixed-effects meta-regression models. A simulation study was carried out to compare different methods in terms of their Type I error and statistical power rates. We included the standard (Wald-type) test, the method proposed by Knapp and Hartung (2003) in 2 different versions, the Huber--White method, the likelihood ratio test, and the permutation test in the simulation study. These methods were combined with 7 estimators for the amount of residual heterogeneity in the effect sizes. Our results show that the standard method, applied in most meta-analyses up to date, does not control the Type I error rate adequately, sometimes leading to overly conservative, but usually to inflated, Type I error rates. Of the different methods evaluated, only the Knapp and Hartung method and the permutation test provide adequate control of the Type I error rate across all conditions. Due to its computational simplicity, the Knapp and Hartung method is recommended as a suitable option for most meta-analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {heterogeneity estimator,Meta Analysis,meta-analysis,meta-regression,moderator analysis,standardized mean difference,Statistical Estimation,Statistical Power,Testing,Type I Errors},
  file = {C:\Users\micro\Zotero\storage\KN2TDI9F\Viechtbauer et al. - 2015 - A comparison of procedures to test for moderators .pdf}
}

@article{zhang2012,
  title = {An {{Approximate Hotelling T2-Test}} for {{Heteroscedastic One-Way MANOVA}}},
  author = {Zhang, Jin-Ting},
  year = {2012},
  month = jan,
  journal = {Open Journal of Statistics},
  volume = {2},
  number = {1},
  pages = {1--11},
  publisher = {Scientific Research Publishing},
  doi = {10.4236/ojs.2012.21001},
  urldate = {2024-10-30},
  abstract = {In this paper, we consider the general linear hypothesis testing (GLHT) problem in heteroscedastic one-way MANOVA. The well-known Wald-type test statistic is used. Its null distribution is approximated by a Hotelling T2 distribution with one parameter estimated from the data, resulting in the so-called approximate Hotelling T2 (AHT) test. The AHT test is shown to be invariant under affine transformation, different choices of the contrast matrix specifying the same hypothesis, and different labeling schemes of the mean vectors. The AHT test can be simply conducted using the usual F-distribution. Simulation studies and real data applications show that the AHT test substantially outperforms the test of [1] and is comparable to the parametric bootstrap (PB) test of [2] for the multivariate k-sample Behrens-Fisher problem which is a special case of the GLHT problem in heteroscedastic one-way MANOVA.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\F4WWT6GE\Zhang - 2012 - An Approximate Hotelling T2-Test for Heteroscedast.pdf}
}

@article{zhang2013,
  title = {Tests of {{Linear Hypotheses}} in the {{ANOVA}} under {{Heteroscedasticity}}},
  author = {Zhang, Jin-Ting},
  year = {2013},
  month = may,
  journal = {International Journal of Advanced Statistics and Probability},
  volume = {1},
  number = {2},
  pages = {9--24},
  issn = {2307-9045},
  doi = {10.14419/ijasp.v1i2.908},
  urldate = {2024-10-30},
  abstract = {It is often interest to undertake~a general linear hypothesis testing (GLHT)problem in the one-way ~ANOVA without assuming the equality of thegroup variances. When the equality of the group variances is valid,it is well known that the GLHT problem~can be solved by the classical F-test. The classical F-test, however, ~may ~lead to misleading conclusions~when the variance homogeneity assumption is seriously violated since it doesnot take the group variance heteroscedasticity into account. To ourknowledge, little work has been done for this heteroscedastic GLHTproblem ~except for some special cases. In this paper, we propose asimple approximate Hotelling T2 (AHT) test. ~We show that the AHTtest is invariant under affine-transformations, different choices ofthe coefficient matrix used to define the same hypothesis, anddifferent labeling schemes of the group means. Simulations and realdata applications indicate that the AHT test is comparable with oroutperforms some well-known approximate solutions proposed for the k-sample Behrens-Fisher problem which is a special case of theheteroscedastic GLHT problem.},
  langid = {english},
  file = {C:\Users\micro\Zotero\storage\Z7IPIT8N\Zhang - 2013 - Tests of Linear Hypotheses in the ANOVA under Hete.pdf}
}

@article{zhang2024,
  title = {Statistical {{Power Analysis}} for {{Univariate Meta-Analysis}}: {{A Three-Level Model}}},
  shorttitle = {Statistical {{Power Analysis}} for {{Univariate Meta-Analysis}}},
  author = {Zhang, Bixi and Konstantopoulos, Spyros},
  year = {2024},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {0},
  number = {0},
  pages = {1--28},
  publisher = {Routledge},
  issn = {1934-5747},
  doi = {10.1080/19345747.2023.2290544},
  urldate = {2024-10-31},
  abstract = {This study extends prior work on power analysis in two-level meta-analysis and provides methods on power analysis for univariate three-level meta-analysis. In a three-level hierarchical structure effect sizes are nested within studies, which in turn are nested within research groups of investigators. Consequently, the three-level model takes into account the between-study (second level) and the between-research group (third level) variances. The study provides closed form equations to compute the prospective statistical power of the z-test of the combined weighted average effect size. Illustrative examples that demonstrate the applicability of the methods in prospective power analysis are provided. Moreover, a simulation study is also conducted to demonstrate how prospective statistical power is modified by changes of the values of the design parameters that impact power within the context of the three-level model. Results indicate the third-level variance is inversely related with statistical power in three-level meta-analysis.},
  file = {C:\Users\micro\Zotero\storage\F2T6V5XE\Zhang and Konstantopoulos - 2024 - Statistical Power Analysis for Univariate Meta-Ana.pdf}
}

@misc{zotero-4807,
  title = {{{https://www.stat.cmu.edu/{\textasciitilde}hseltman/309/Book/chapter12.pdf}}},
  urldate = {2024-02-26},
  howpublished = {https://www.stat.cmu.edu/{\textasciitilde}hseltman/309/Book/chapter12.pdf}
}

@misc{zotero-4808,
  title = {{{https://www.stat.cmu.edu/{\textasciitilde}hseltman/309/Book/chapter12.pdf}}},
  urldate = {2024-02-26},
  howpublished = {https://www.stat.cmu.edu/{\textasciitilde}hseltman/309/Book/chapter12.pdf}
}

@misc{zotero-4832,
  title = {{{https://matthewbjane.quarto.pub/pre-post-correlations/index.pdf}}},
  urldate = {2024-07-19},
  howpublished = {https://matthewbjane.quarto.pub/pre-post-correlations/index.pdf}
}

@misc{zotero-4955,
  title = {Conducting Power Analysis for Meta-analysis with Dependent Effect Sizes: {{Common}} Guidelines and an Introduction to the {{POMADE R}} Package - {{Vembye}} - {{Research Synthesis Methods}} - {{Wiley Online Library}}},
  urldate = {2024-10-31},
  howpublished = {https://onlinelibrary-wiley-com.ezproxy.lib.utexas.edu/doi/full/10.1002/jrsm.1752},
  file = {C:\Users\micro\Zotero\storage\X9C2DYAP\jrsm.html}
}

@misc{zotero-4957,
  title = {Self-Regulated Learning Partially Mediates the Effect of Self-Regulated Learning Interventions on Achievement in Higher Education: {{A}} Meta-Analysis - {{ScienceDirect}}},
  urldate = {2024-10-31},
  howpublished = {https://www-sciencedirect-com.ezproxy.lib.utexas.edu/science/article/pii/S1747938X18304342},
  file = {C:\Users\micro\Zotero\storage\G3YTRHFZ\S1747938X18304342.html}
}
