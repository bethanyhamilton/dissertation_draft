\chapter{Discussion}\label{ch: discussion}

In meta-analysis, multiple dependent effect size estimates in primary studies are prevalent, and methods that account for the dependence are widely used \autocite{hedges2010, pustejovsky2022, vandennoortgate2013, vandennoortgate2015, betsy_jane_becker_model}. A primary goal of meta-analysis is to explain heterogeneity of effect sizes through meta-regression. One way to do this is to include categorical moderators, a common type of covariate in social science meta-analytic research \autocite{tipton2019a}. With the development and wide use of working models that account for more than one source of dependence \autocite{pustejovsky2022}, a methodology for conducting power analysis for the tests of such models is needed for obtaining \textit{a priori} power estimates. Due to the complexity of these models, it is important to consider the performance of these models when planning a systematic review. Furthermore, using univariate \textit{a priori} power analysis methods \autocite{hedges2001} for a meta-analysis involving dependent effect sizes results in inaccurate power estimates \autocite{vembye2023}.  

\textcite{vembye2023} developed and evaluated approximation formulas for the power of the test of the overall pooled effect for the MLMA, CE, and CHE working models. I extended this work in my dissertation by developing and evaluating a closed-form approximation for the HTZ tests of study-level categorical moderators for the CHE model.  

Below, I first discuss the implications of the results of this dissertation.  Then, I detail the limitations of this study and offer future directions.  Finally, I offer some guidelines for applied researchers.

\section{Implications}

% The primary conclusion is that the approximation is accurate except for a high number of categories and small degrees of freedom. Something that can be calculated in advance. Do power calculation in advance and look at the degrees of freedom; if very small, then maybe don't trust the result.

I had three aims for my study: 1) To validate the approximation for the power of the Wald test of a categorical study-level moderator from the CHE model with HTZ degrees of freedom through a simulation; 2) To compare methods for assuming sampling variances and the number of effect sizes in the power approximation formula; and 3) To evaluate Type I Error for the robust-Wald Test from a CHE working model. From the results of this study, I conclude that 1) The power approximation formula is accurate when there are a small number of contrasts, but it can be inaccurate in conditions with a larger number of contrasts and small degrees of freedom; 2) It is important to find reliable pilot data for sampling the number of the effect sizes and the sampling variances, as the accuracy of the formula did depended on how closely the assumed distributions matched the data generating distributions; and 3) The Type I Error of the robust-Wald Test from a CHE working model was below the nominal error rate, especially in conditions with more contrasts and a small number of total studies. I expand on these conclusions below.  

For the first aim, under an ideal situation where the primary study characteristics for the power approximation were sampled from the same dataset used to generate the simulated data, the results showed that the power estimates of the approximation formula are accurate for the true power of the robust-Wald test for the CHE model when there were a smaller number of categories ($C \leq 3$) and in cases where there were a larger number of categories ($C = 4$) and large meta-analytic datasets ($J \geq 60$). However, under some conditions, when there were more categories and a small to moderate total number of studies ($J \leq 48$), the approximation formula overestimated the true power by as much as $21$ percentage points. Furthermore, the balance of the number of studies across categories and the between-study heterogeneity impacted the magnitude of the power discrepancy, with an imbalance in the number of studies across categories and smaller between-study heterogeneity resulting in the biggest power discrepancies. The approximation never underestimated true power. 

%Under the conditions I evaluated and the sampling techniques for the $k_j$ and $\sigma_j^2$, the approximation never underestimated the true power. Furthermore, across all conditions except for a few when $C=4$, the approximation power estimates were within the threshold of $5\%$ of the true power. However, I did not assess the mis-specification of these values. 

One possible reason that the approximation overestimated the true power could be related to the findings of my third aim, where I found that the simulated Type I error rates for the cluster-robust Wald test of the study-level categorical moderator are conservative when the number of contrasts ($q$) is greater and when the total number of studies is small. These findings are consistent with those of \textcite{tipton2015b} and \textcite{joshi_cluster_2022}; they also found that while none of the factors yielded Type I error rates substantially above nominal levels, the HTZ test has more conservative Type I error rates and becomes even more conservative when there are more contrasts. While the HTZ test performs well at controlling Type I error, the power of the test is penalized when the number of contrasts, $q$, increases. The reason is that as $q$ increases, the multiplier of the $Q$ to equate it to an $F$ ($[(\eta_z - q +1)/(\eta_zq)]$) and the denominator degrees of freedom of the $F$-statistic ($d_2 = \eta_z - q +1$) decrease \autocite{tipton2015b}. 

While this conservatism in the Type I error rate lowers the likelihood of making a Type I error, it also lowers statistical power. The low power is attributable to the test being conservative, which could be why the approximation overstates the power. Perhaps the power of a more appropriately calibrated method for the Wald test, like the cluster wild bootstrap test (CWB) examined by \textcite{joshi_cluster_2022}, may be more aligned with the power estimates of the approximation.  
\textcite{joshi_cluster_2022} examined CWB as an alternative small-sample correction test to the HTZ test, and they found that CWB controls for Type I error and provides more power than the HTZ test, especially for tests of multiple contrasts. A future study validating the proposed approximation for the power of the CWB test is worth conducting, because the CWB is computationally intensive and it would be useful to be able to estimate power for a proposed analysis that involves CWB. 

In my results, I also found that the magnitude of the degrees of freedom is a useful diagnostic indicator for whether power approximations are accurate for the HTZ Wald test. Specifically, a small degrees of freedom and more contrasts could indicate that the power approximation will overestimate the true power and a researcher should then be more cautious about the associated power estimate (see Figure \ref{fig: df_mean}). However, if the number of contrasts is smaller ($q \leq 2$) or if the number of contrasts is greater ($q \geq 3$) and there is a larger degrees of freedom value ($d_2 \geq 12$), then the approximation will be reliable. When conducting an \textit{a priori} power analysis, a meta-analyst can look at the degrees of freedom to determine whether they can trust the results. However, it is important to note, a small degree of freedom does not guarantee a large discrepancy between actual and estimated power estimates; there were a large number of samples that had small degrees of freedom due to an imbalanced number of studies across categories, small between-study heterogeneity, four categories, and/or a small total number of studies, where the approximation was accurate. Additionally, the balance of the number of studies across categories and between-study heterogeneity also impacted the magnitude of the degrees of freedom. \textcite{tipton2015b} found that degrees of freedom in a meta-regression using HTZ were smaller for imbalanced covariates as well. Finally, if there is an interest in finding the power of the HTZ test for a larger number of contrasts and the magnitude of the degrees of freedom is small, then it may be necessary instead to simulate the power of the test as I did in this study. 

Regarding the findings of my third aim independently, I was able to replicate the findings of \textcite{joshi_cluster_2022} and \textcite{tipton2015b} through a distinctly different way of simulating the design matrices and vector of $\beta$ meta-regression coefficients. Both studies used different covariate specifications that were more limited in scope. The design matrix that \textcite{tipton2015b} used in their simulation study had five covariates of various types, including one binary study-level moderator with extreme imbalance in the number of studies across the categories. They evaluated a different number of contrasts, which resulted in 26 unique coefficient vectors. In their first study, \textcite{joshi_cluster_2022} evaluated the HTZ test with the same design matrix as \textcite{tipton2015b} and 11 unique coefficient vectors. For their second study, \textcite{joshi_cluster_2022} generated a single categorical covariate as I did, but its value varied either at the study-level or effect-size level. The categorical covariate had either 3, 4, or 5 categories. For the $\beta$ coefficients, they fixed the intercept to 0.3 and varied only the first slope with three values, which resulted in 24 unique coefficient vectors. For my study, the design matrices only included study-level categorical moderators, and I induced imbalance in the number of studies across the categories through fixed proportions depending on the number of categories. Also, by generating the $\mu_c$ values through data-generating conditions, I evaluated $7,203$ unique coefficient vectors (See Figure \ref{fig:max_mu} for the maximum $\beta$ estimate value of each coefficient vector). I was also able to replicate the findings of \textcite{joshi_cluster_2022} and \textcite{tipton2015b} using a different working model than they used in their simulation studies.  \textcite{joshi_cluster_2022} used a correlated effects working model where \textcite{tipton2015b} used a fixed effects working model. I replicated their findings under a wider range of design matrices, regression coefficients, and a more general working model.   

For the conclusions of my second aim, I found that the approximate power estimates that result from sampling primary study characteristics from a stylized distribution followed the same pattern as those that result from sampling within-study characteristics from an empirical distribution. Neither sampling method performed well when $C=4$, and there was a small to moderate number of studies. However, the empirical sampling method performed slightly better with a maximum overestimation of approximately $21$ percentage points compared to that of the stylized method's approximately $26$ percentage point overestimation. Also, when the number of contrasts was small, the stylized method did overestimate the true power by more than five percentage points in some conditions ($C = 3$ and $J \leq 36$) by as much as approximately $9$ percentage points. As \textcite{vembye2023} noted, since reliable pilot data is not always available for an \textit{a priori} power analysis, meta-analysts can use stylized distributions of $k_j$ and $\sigma_j^2$ for the approximation. In practice, I suggest factoring in that using a stylized distribution for the sampling method of the primary study characteristics could overestimate the true power more than reliable pilot data. 

Assuming that the primary study characteristics are balanced resulted in substantially underestimated power estimates for the Wald test with the CHE+RVE model across all conditions except when $C=4$, and there were a small to moderate number of studies where it also overestimated the true power. For the balanced assumption method, the true power was at most approximately $25$ percentage points above and $24$ percentage points below a power of $60\%$.  Therefore, I do not recommend that meta-analysts use the balanced method in practice. 

\section{Limitations and Future Directions}

Below, I detail limitations of this study that should be considered to determine the extent to which these results can be generalized. I also highlight areas where this study can be extended. 

The proposed approximation of the cluster-robust Wald test only applies to study-level categorical moderators. I first focused on categorical moderators because they are commonly used in meta-analytic practice \autocite{ahn2012, tipton2019}. Additional work is needed to develop approximations for within-study categorical and between-study and within-study continuous moderators. As \textcite{vembye2023} noted, and I also faced in this dissertation study, it is necessary to make assumptions about the distribution of the covariate across studies and effect sizes. That can be challenging in practice. In future research, the balance of the number of effect sizes and the number of studies across covariates should also be considered for within-study moderators.  

The proposed power approximation for the robust-Wald test of study-level categorical moderators was developed for data structures that follow the CHE model, while \textcite{vembye2023} developed and evaluated approximations for the CE and MLMA working models for the test of the overall pooled effect as well. For my purposes, I chose not to look at special cases of the CHE model where there is no within-study variance or no correlation among effect sizes because meta-analytic data often has both sources of dependence \autocite{pustejovsky2022}. Because it is possible for data structures to have only one type of dependence, though, it may be worthwhile to develop approximations for these special cases. Additionally, the proposed approximation should be validated when the simulated data follows a different working model (such as $\rho=0$ for the MLMA model or $\omega=0$ for the CE model).

Furthermore, this approximation only proposed HTZ-based degrees of freedom, not model-based degrees of freedom, nor other robust tests such as the Eigen-decomposition F-test or the Eigen-decomposition and transformation test \autocite{tipton2015b}. I decided to first focus on developing the HTZ-based degrees of freedom for RVE because \textcite{vembye2023} found that the approximations that used RVE were more accurate than the model-based ones, where model-based tests had inflated Type I error for the test of the overall pooled effect. Also, the model-based degrees of freedom will have close to-correct Type I error only when the working model is correctly specified \autocite{vembye2023}. For the eigen-decomposition-based methods, \textcite{tipton2015b} found that they also resulted in high Type I error rates. 

Additionally, for the simulated data, I assumed that the effect sizes in each study and across studies were equally correlated, which may not accurately reflect the reality in which the sampling correlation could vary across studies. Also, it could be that variability in sampling correlation impacts the power of the true estimates, and therefore, also affects the discrepancy between the true and approximated power. Future studies could generate a sampling correlation that varies between studies by assuming it follows a $Beta$ distribution as was done in \textcite{tipton2015b, joshi_cluster_2022, vembye2023}. Another next step would be to examine when the sampling correlation used in the data-generating process is different from the sampling correlation used in the estimation or the approximation formula.

For this study, I only used one empirical distribution of $k_j$ and $\sigma_j^2$ to validate the approximation. Further steps should be taken to validate the approximation with other distributions of $k_j$ and $\sigma_j^2$ to test whether the approximation results in more inaccurate power estimates. Another limitation of this study is that the distributions of $k_j$ and $\sigma_j^2$ for the stylized sampling method were pretty similar to those of the empirical distributions. Further work is needed to evaluate how robust the approximation is to using estimates of $k_j$ and $\sigma_j^2$ drawn from a distribution quite different from that of the data-generating sample. 

When assuming balanced study characteristics, further work is needed to evaluate the use of imputing other numbers besides the arithmetic mean of the empirical distribution for the balanced method, specifically for assuming values for $\sigma_j^2$ and $k_j$ in the approximation. Alternatives include using the harmonic mean of $\sigma_j^2$, which will lead to somewhat larger studies overall, or computing the weights for an entire empirical distribution given the design characteristics $\tau^2$, $\omega^2$, and $\rho$ and imputing the average of the weights. 

The conclusions of this study are limited to the study design conditions examined (Table \ref{tab:experimentalconditions}). For example, the smallest value for the total number of studies factor that I evaluated was $24$. Still, many applied researchers have meta-analytic datasets with an even smaller total number of studies. Furthermore, I only looked at a maximum of four categories when a categorical moderator could have many more categories in practice. Additionally, because there are an infinite number of possibilities, specifying alternative hypotheses for multiple contrasts can be challenging. The patterns of the $\mu_c$ were my first attempt to specify possible alternative hypotheses, but this can be refined in future studies. I believe a review of meta-analyses on the number of categories, the number of studies per category, and the distribution of regression slopes is needed to create more design conditions to test this approximation. 

I simulated standardized mean differences; therefore, given the distributional similarities, these results can also be applied to Fisher-Z transformed correlation effect sizes. Still, the results will not translate to other effect size types like odds ratio without further assumptions \autocite{vembye2023}.

Finally, further work is needed to develop a paper that demonstrates the practical application of the power approximation, as was done in \textcite{vembye2024}. Additionally, to make the approximation more accessible, it should be implemented in an R software package such as \texttt{POMADE} \autocite{POMADE}.

\section{Guidelines for Applied Researchers}
%The only suggestion that your committee had was to include a short section in the discussion section offering guidelines to applied meta-analysts. Explaining how to use the different degrees of freedom, handling when a categorical variable might have too many categories, and how that impacts the power assessment and Jamesâ€™ last question about solving for the mus per category and why these might be of use to a meta-analyst.

%%%%%%%%%%%%%%%%%%%%%%%%%

Power analysis in the context of primary studies is used to determine the sample size required to achieve a certain power level in a test of a sample. In the context of meta-analysis, however, the goal is to include all, rather than a sample of, studies available on a topic. For the power analysis of a study-level categorical moderator, there are three applications of the proposed approximation formula that might be useful to applied meta-analysts. The approximation formula can be used to 1) obtain power estimates for the HTZ test, 2) determine the minimal detectable $\mu_c$ values, or 3) determine the required number of studies to have sufficient power to find differences as a function of the variables of interest. 

Power analysis for meta-regression is useful for determining whether a moderator analysis will be adequately powered when planning or for better understanding the performance of the analysis model. In addition to using for \textit{a priori} calculations for planning or justifying a meta-analysis, the approximation can also be used after collecting meta-analytic data when the primary study characteristics are known. The approximation can also be used after running a summary meta-analysis (i.e., finding the overall pooled effect across all effect sizes), when the value for the degree of heterogeneity is known. Applying the approximation in these scenarios could save researchers time conducting underpowered moderator analyses if they can specify upfront whether they were sufficiently powered to detect differences as a function of the variables of interest. As another possible idea, the approximation could be used to update an \textit{a priori} power analysis at different stages of the meta-analysis.

%Suppose power is a concern due to a high number of contrasts and a smaller number of studies. Start by conducting an \textit{a priori} analysis, but once you have collected the data, before the moderator analyzes it, you can update the power analysis with the known parameters. By doing this, you can save time on moderator analyses and determine whether you have sufficient power to find differences in the variables of interest.  

%-Go over finding power. What assumptions? 

% One high-level narrative description should apply this -- and then call a day. Should not be in the weeds. Lean on the Vembye application paper for everything you can. Comment on pieces that are distinct based on the F-test, specifically the number of categories and configuration of average effects. Balance of studies across categories. 

%In general, though, for power analyses, it is good to conduct a sensitivity analysis and test many different conditions if possible. 

%The following is for an applied researcher who assumes they have an idea of the number of categories and the overall pooled effect size per category, as well as the number of studies they expect to find, and wants to see the resulting power for the HTZ test. 

As I presented in Section \ref{sec: power_multiple}, power analysis using this approximation formula depends on the non-centrality parameter, the degrees of freedom for the non-central F distribution, and the critical value. The formulae I propose for the non-centrality parameter (Equation \ref{eq: NCP}) and the degrees of freedom for the non-central F distribution involve specifying values for the overall pooled effect for each category of the moderator variable and study-level weights (Equation \ref{eq: CHEweights-study_cat}). 

For the study-level weights, if not available, assumptions must be made about the number of effect sizes per study ($k_j$), sample variances ($\sigma_j^2$), sample correlation ($\rho$), between-study heterogeneity ($\tau$), within-study heterogeneity ($\omega$), number of studies per category ($J_c$), and the balance of the number of studies across the categories ($bal. j_c$). In this dissertation, I evaluated methods for sampling the $k_j$ and $\sigma_j^2$ to assess the impact on results. I found that the accuracy of the approximation depends on how similar the assumed distributions for these study characteristics are to those of the projected distributions of these study characteristics of the target meta-analytic dataset. I recommend that applied meta-analysts collect pilot data or attempt to obtain these values from an empirical meta-analysis study that is very similar to the one being conducted to derive \textit{a priori} power estimates. I also suggest that applied meta-analysts not assume balanced primary study characteristics. If balance is assumed incorrectly, I found that this resulted in large inaccuracies in the power approximation results. For the values of $\rho$, $\tau$, $J$, and $\omega$, a sensitivity analysis can be conducted to evaluate the impact of different assumptions and their corresponding values. As \textcite{vembye2024} suggested, it is also worth looking into reviews of meta-analyses in similar topics from a relevant field to inform reasonable values for these parameters. If the applied researcher is uncertain about the balance of studies across categories ($bal. j_c$), I recommend assessing the impact of both a balanced and an extreme imbalance in the number of studies across categories on power. I found that an imbalanced number of studies across categories impacted power, so a safe approach would be to account for imbalance in the power analyses. 

%There might be some follow-up analysis to do here (for an applied guidance paper) to figure out which configuration minimizes the NCP.

%Describing the $\mu_c$ as the pattern and maximal difference. To use the method in practice. How big of the difference is the smallest largest of the categories. I looked at these patterns, but you can specify your own...


%researchers to specify 1) a configuration/pattern and 2) a maximum difference between categories. Or if solving for minimum detectable ES, one could specify a configuration / pattern and then solve for the NCP to see what the specific values would need to be

Finding power using the approximation also involves specifying values for the meta-regression coefficients. For our purposes, I operationalized the meta-regression coefficients as the overall average effect for each category of the study-level moderator. However, it should be noted that when using the approximation, it is not necessary to specify the overall average effect for each category, and the approximation formula and NCP will work using the slope estimates from a model with an intercept as well. I also came up with different alternative hypothesis scenarios ($f_c$). Suppose an applied researcher's goal is to find power or the required number of studies using the approximation. In that case, it is necessary to specify the expected values for the meta-regression coefficients. The alternative hypothesis will be related to the research question and reviews of past research on the moderator. I evaluated many such alternative hypotheses through different patterns for the $\mu_c$ ($f_c$; see Table \ref{tab:patterns}). For example, I looked at four patterns for a moderator of four categories: 1) all of the $\mu_c$ values were equally spaced and different 2) only two categories were different 3) only one category differs from the others, and 4) half of the categories had one value and half of the categories had another value. I defined the $\mu_c$ values as the pattern multiplied by a scaling factor.  To apply this method in practice, defining the $\mu_c$ as the pattern and the maximally realistic difference between the largest and the smallest categories would be useful. I also suggest conducting a sensitivity analysis across different configurations and patterns to try to determine the pattern that produces the lowest power (lowest NCP). Alternatively, if available and of interest, a researcher could utilize $\mu_c$ values from past research or reviews in a given field and topic that they wanted to test. If, instead, an applied researcher is interested in determining the minimal detectable $\mu_c$ values given a specified power level and expected number of studies, then it is still necessary to specify the alternative hypotheses scenarios that they would expect. In this framework, they could find a range of possible values for the meta-regression coefficients given the other design parameters. 

%Because it is necessary to calculate degrees of freedom in this process, there is a step in the power analysis that an applied researcher should consider. 

Finally, I found that if the moderator of interest has four categories or more, then the accuracy of the approximation depends on the magnitude of the degrees of freedom. As stated earlier, with a smaller value for the degrees of freedom $(d_2 < 12)$, my results suggest that the approximation may potentially overestimate the true power of the test, and the approximation formula's results may be unreliable. I suggest not using the approximation formula under such circumstances.

\section{Conclusions}

An \textit{a priori} power analysis helps meta-analysts and potential funders determine whether a number of studies is large enough to detect an effect size of practical importance. Additionally, \textit{a priori} power analysis helps in planning a confirmatory research project when developing the analytic methodology. Applied researchers could conduct a power analysis for a meta-regression through a Monte Carlo simulation. However, such an analysis is not always accessible and takes time to develop. Having the approximation formula available makes the \textit{a priori} power analysis more straightforward.







